{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "677d303e",
   "metadata": {},
   "source": [
    "## Pipeline de Consulta Semântica com RAG e LLMs Locais (Documentos SEI UTFPR)\n",
    "\n",
    "Este pipeline foi projetado para **responder a perguntas sobre o conteúdo de documentos HTML do SEI UTFPR** de forma inteligente, utilizando a arquitetura de Recuperação Aumentada por Geração (RAG). Ele combina a **busca semântica em um banco vetorial ChromaDB** (contendo embeddings de documentos SEI) com a capacidade de **geração de respostas de Large Language Models (LLMs) locais**, otimizando a recuperação e a síntese de informações.\n",
    "\n",
    "---\n",
    "\n",
    "### Ferramentas Essenciais\n",
    "\n",
    "* **sentence-transformers**: Para a criação de embeddings da consulta do usuário e o carregamento dos modelos de embedding.\n",
    "* **ChromaDB**: Banco de dados vetorial para armazenamento e busca eficiente das informações.\n",
    "* **transformers**: Para o carregamento e uso de Large Language Models (LLMs) locais na geração de respostas.\n",
    "* **dotenv** e **huggingface_hub**: Para gerenciamento de autenticação no Hugging Face e variáveis de ambiente.\n",
    "* **torch**: Biblioteca para computação tensorial e gerenciamento de hardware (GPU/CPU).\n",
    "\n",
    "---\n",
    "\n",
    "### Estratégia de Operação\n",
    "\n",
    "1.  **Autenticação e Configuração Inicial**:\n",
    "    * **Login no Hugging Face**: Autenticação via token para acesso a modelos restritos.\n",
    "    * **Configuração do ChromaDB**: Inicialização do cliente persistente do ChromaDB, apontando para o diretório onde as coleções de embeddings estão armazenadas.\n",
    "\n",
    "2.  **Seleção Dinâmica do Modelo de Embedding**:\n",
    "    * O pipeline apresenta uma lista de **modelos de embedding disponíveis** (ex: `all-MiniLM-L6-v2`, `bert-base-portuguese-cased`).\n",
    "    * O usuário escolhe qual modelo de embedding deseja utilizar para a consulta atual.\n",
    "    * O **modelo de embedding e a coleção correspondente no ChromaDB são carregados sob demanda**, otimizando o uso de recursos de memória, pois apenas o necessário é mantido ativo.\n",
    "\n",
    "3.  **Carregamento do LLM (Local)**:\n",
    "    * Um **LLM local** (ex: `gemma-2b`, `llama-3.2-1b`) é carregado e permanece em memória.\n",
    "    * A configuração de quantização (4-bit) com offload para CPU é utilizada para **otimizar o consumo de memória**, permitindo a execução em ambientes com recursos limitados de GPU.\n",
    "\n",
    "4.  **Recuperação de Informações (RAG)**:\n",
    "    * **Embeddings da Consulta**: A pergunta do usuário é convertida em um embedding usando o modelo de embedding selecionado.\n",
    "    * **Extração de Filtros**: O pipeline tenta **identificar automaticamente termos nos metadados** (ex: `tipo_documento`, `órgão`, `data_publicação`, `título`) presentes na pergunta do usuário. Esses termos são usados como **filtros de metadados** para refinar a busca no ChromaDB.\n",
    "    * **Busca no ChromaDB**: A busca é realizada no ChromaDB utilizando o embedding da consulta e os filtros de metadados extraídos. Isso garante que os **chunks mais relevantes e contextualmente alinhados** à pergunta sejam recuperados dos documentos SEI.\n",
    "\n",
    "5.  **Geração de Resposta**:\n",
    "    * **Criação do Prompt**: Os chunks recuperados servem como **contexto** para a construção de um prompt para o LLM. O prompt instrui o LLM a responder \"APENAS com as informações fornecidas no contexto\", atuando como um assistente especializado em documentos oficiais da UTFPR.\n",
    "    * **Inferência do LLM**: O LLM local gera uma resposta coerente e concisa com base no prompt e no contexto fornecido, evitando informações externas ou \"alucinações\".\n",
    "\n",
    "---\n",
    "\n",
    "### Benefícios\n",
    "\n",
    "* **Flexibilidade e Comparação**: Permite testar e comparar a performance de diferentes modelos de embedding em tempo real.\n",
    "* **Precisão Contextual**: Utiliza filtros inteligentes baseados em metadados para garantir que as respostas sejam altamente relevantes ao contexto da pergunta sobre os documentos SEI.\n",
    "* **Eficiência de Recurso**: Otimiza o uso de memória ao carregar modelos de embedding sob demanda e usar quantização para o LLM.\n",
    "* **Transparência**: Exibe os chunks recuperados, seus metadados e o prompt enviado ao LLM, proporcionando visibilidade sobre o processo de recuperação e geração.\n",
    "* **Controle de Informação**: As respostas são estritamente baseadas no contexto recuperado dos documentos SEI, garantindo factualidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1: Importando bibliotecas\n",
    "\n",
    "# Bibliotecas padrão\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Bibliotecas de dados e visualização\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Bibliotecas de IA e embeddings\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Bibliotecas de NLP e ML\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Bibliotecas para avaliação RAGAS\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
    "from ragas import evaluate\n",
    "import ragas\n",
    "from datasets import Dataset\n",
    "\n",
    "# nltk.download('all') # Baixar recursos do NLTK (descomente se for a primeira execução)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0881a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2: Login no Hugging Face\n",
    "\n",
    "# Carregar variáveis de ambiente do arquivo .env\n",
    "load_dotenv()\n",
    "KEY = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "# Fazer login no Hugging Face\n",
    "from huggingface_hub import login\n",
    "login(token=KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d157047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3: Configuração do ChromaDB e mapeamento de modelos\n",
    "\n",
    "# Configurar ChromaDB\n",
    "CHROMA_DIR = \"chroma_db\"\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DIR)\n",
    "\n",
    "# Mapeamento dos nomes das coleções para os nomes corretos dos modelos Hugging Face\n",
    "EMBEDDING_MODEL_MAP = {\n",
    "    \"all-MiniLM-L6-v2\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"all-mpnet-base-v2\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    \"paraphrase-multilingual-MiniLM-L12-v2\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"distiluse-base-multilingual-cased-v2\": \"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "    \"stsb-xlm-r-multilingual\": \"sentence-transformers/stsb-xlm-r-multilingual\",\n",
    "    \"neuralmind-bert-base-portuguese-cased\": \"neuralmind/bert-base-portuguese-cased\"\n",
    "}\n",
    "\n",
    "def load_embedding_model_and_collection(model_key: str):\n",
    "    \"\"\"\n",
    "    Carrega o modelo de embedding e a collection correspondente.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model_path = EMBEDDING_MODEL_MAP[model_key]\n",
    "        collection_name = f\"documentos-SEI__{model_key}\"\n",
    "\n",
    "        print(f\"\\nCarregando modelo: {model_path}\")\n",
    "        embedding_model = SentenceTransformer(model_path, device=device)\n",
    "\n",
    "        print(f\"Carregando collection: {collection_name}\")\n",
    "        collection = chroma_client.get_collection(collection_name)\n",
    "\n",
    "        print(f\"Collection carregada com {collection.count()} documentos.\")\n",
    "        return embedding_model, collection\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar modelo/collection: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b6830",
   "metadata": {},
   "source": [
    "### Configurações Recomendadas para Carregamento de Modelos\n",
    "\n",
    "Configurações recomendadas para carregar diferentes modelos de linguagem (LLMs) usando a biblioteca `transformers` do Hugging Face, visando otimizar compatibilidade, desempenho e estabilidade.\n",
    "\n",
    "#### LLaMA, Gemma e outros modelos com quantização 4-bit\n",
    "\n",
    "- **Configuração:**\n",
    "  - `quantization_config = BitsAndBytesConfig(...)`: Configuração para quantização 4-bit, com parâmetros:\n",
    "    - `load_in_4bit=True`: Ativa a quantização 4-bit.\n",
    "    - `bnb_4bit_compute_dtype=torch.float16`: Define o tipo de dado para cálculos internos.\n",
    "    - `llm_int8_enable_fp32_cpu_offload=True`: Permite offload para CPU em float32 para estabilidade.\n",
    "  - `tokenizer = AutoTokenizer.from_pretrained(model_id)`: Carrega o tokenizer do modelo.\n",
    "\n",
    "  - `model = AutoModelForCausalLM.from_pretrained(...)`: Carrega o modelo com:\n",
    "    - `device_map=\"auto\"`: Distribui o modelo automaticamente entre CPU/GPU.\n",
    "    - `torch_dtype=torch.float16`: Usa half-precision para acelerar a inferência.\n",
    "    - `quantization_config=quantization_config`: Aplica a quantização 4-bit.\n",
    "    - `trust_remote_code=True`: Permite execução de código customizado do repositório.\n",
    "\n",
    "- **Referências:**\n",
    "  - [Documentação do `transformers` sobre quantização 4-bit](https://huggingface.co/docs/transformers/main/en/main_classes/quantization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4: Carregar o LLM\n",
    "AVAILABLE_MODELS = {\n",
    "    \"llama-3.2-1b\": \"meta-llama/Llama-3.2-1B\",\n",
    "    \"gemma-2b\": \"google/gemma-2b-it\", # Gemma 2B Original\n",
    "    \"phi-3-mini\": \"microsoft/Phi-3-mini-4k-instruct\" # Phi-3-mini\n",
    "}\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    if model_name not in AVAILABLE_MODELS:\n",
    "        raise ValueError(f\"Modelo {model_name} não disponível. Escolha entre: {list(AVAILABLE_MODELS.keys())}\")\n",
    "    model_id = AVAILABLE_MODELS[model_name]\n",
    "    print(f\"\\nCarregando modelo: {model_id}\")\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=quantization_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "# Carregando o LLM (este permanece carregado pois é usado em todas as consultas)\n",
    "selected_model = \"llama-3.2-1b\"  # Escolha o modelo desejado\n",
    "tokenizer, llm_model = load_model(selected_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c27b474",
   "metadata": {},
   "source": [
    "### Estratégia do Pipeline RAG: Detecção Automática de Filtros e Geração Especializada para Documentos Institucionais\n",
    "\n",
    "#### 1. Extração Dinâmica de Filtros (`extract_metadata_filters`)\n",
    "A função analisa a pergunta e os metadados disponíveis para criar filtros automáticos.\n",
    "\n",
    "* **Amostragem Inteligente:** Obtém uma amostra de documentos para identificar os campos de metadados disponíveis.\n",
    "* **Detecção Contextual:** Verifica se termos dos metadados como 'tipo_documento', 'órgão', 'data_publicação' e 'título' aparecem na pergunta do usuário.\n",
    "* **Composição Lógica:** Cria filtros simples ou compostos (usando operadores `$eq` e `$and`) dependendo da quantidade de correspondências encontradas.\n",
    "\n",
    "#### 2. Recuperação com Filtros Automáticos (`retrieve_relevant_chunks`)\n",
    "Com os filtros identificados, a busca combina relevância semântica e filtragem por metadados.\n",
    "\n",
    "1. **Busca Híbrida:** Aplica simultaneamente a busca por similaridade vetorial e os filtros de metadados detectados.\n",
    "2. **Mecanismo de Fallback:** Se a busca com filtros falhar, recorre automaticamente à busca puramente semântica.\n",
    "3. **Resultado:** Retorna os documentos mais relevantes que satisfazem tanto a semântica da pergunta quanto os critérios específicos mencionados.\n",
    "\n",
    "#### 3. Geração Especializada para Documentos Oficiais (`create_prompt` e `generate_response`)\n",
    "O sistema gera respostas adaptadas ao contexto institucional.\n",
    "\n",
    "* **Persona Especializada:** Define o assistente como \"especializado em documentos oficiais da UTFPR\".\n",
    "* **Instrução de Aprofundamento:** Inclui orientação para \"entender melhor o que diz o contexto, para fornecer informações extras\".\n",
    "* **Parâmetros de Criatividade Controlada:** Utiliza `temperature=0.7` e `repetition_penalty=1.2` para equilibrar precisão e fluidez nas respostas sobre documentos oficiais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5: Funções RAG para documentos SEI\n",
    "\n",
    "def extract_metadata_filters(query: str, collection, embedding_model) -> dict:\n",
    "    \"\"\"\n",
    "    Extrai possíveis filtros de metadados da query do usuário.\n",
    "    Considera os campos típicos dos metadados dos chunks SEI.\n",
    "    \"\"\"\n",
    "    dummy_embedding = embedding_model.encode(\"dummy texto\").tolist()\n",
    "    sample_results = collection.query(\n",
    "        query_embeddings=[dummy_embedding],\n",
    "        n_results=100\n",
    "    )\n",
    "\n",
    "    filters = []\n",
    "    metadata_fields = ['tipo_documento', 'órgão', 'data_publicação', 'título']\n",
    "\n",
    "    for meta in sample_results['metadatas'][0]:\n",
    "        for field in metadata_fields:\n",
    "            if field in meta:\n",
    "                value = meta[field]\n",
    "                if value and value.lower() in query.lower():\n",
    "                    filters.append({\"$eq\": {field: value}})\n",
    "\n",
    "    if len(filters) == 1:\n",
    "        return filters[0]  # Ex: {\"$eq\": {\"tipo_documento\": \"Portaria\"}}\n",
    "    elif len(filters) > 1:\n",
    "        return {\"$and\": filters}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def retrieve_relevant_chunks(query: str, embedding_model, collection, n_results: int = 5):\n",
    "    \"\"\"\n",
    "    Recupera os chunks mais relevantes do ChromaDB usando similarity search e filtros automáticos.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    where_filter = extract_metadata_filters(query, collection, embedding_model)\n",
    "    if where_filter:\n",
    "        print(\"\\nFiltros detectados:\", where_filter)\n",
    "\n",
    "    try:\n",
    "        if where_filter:\n",
    "            results = collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                where=where_filter,\n",
    "                n_results=n_results,\n",
    "                include=[\"documents\", \"metadatas\"]\n",
    "            )\n",
    "        else:\n",
    "            results = collection.query(\n",
    "                query_embeddings=[query_embedding],\n",
    "                n_results=n_results,\n",
    "                include=[\"documents\", \"metadatas\"]\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na busca: {e}\")\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"metadatas\"]\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "def create_prompt(query: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Cria um prompt para o LLM usando a query e o contexto.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Você é um assistente especializado em documentos oficiais da UTFPR. \"\n",
    "        f\"Responda à pergunta usando APENAS as informações fornecidas no contexto abaixo.\\n\"\n",
    "        f\"Se a informação não estiver disponível no contexto, diga que não pode responder.\\n\"\n",
    "        f\"Se houver múltiplas informações relacionadas, forneça uma resposta completa e organizada.\\n\"\n",
    "        f\"Tente entender melhor o que diz o contexto, para fornecer informações extras.\\n\"\n",
    "        f\"---\\n\"\n",
    "        f\"Contexto:\\n{context}\\n\"\n",
    "        f\"---\\n\"\n",
    "        f\"Pergunta: {query}\\n\"\n",
    "        f\"Resposta:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def generate_response(prompt: str, tokenizer, model, max_length: int = 4096) -> str:\n",
    "    \"\"\"\n",
    "    Gera uma resposta usando o LLM.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        min_length=100,\n",
    "        repetition_penalty=1.2,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.replace(prompt, \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914bb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6: Interface de consulta para documentos SEI (com exibição explícita dos metadados)\n",
    "\n",
    "def select_embedding_model():\n",
    "    \"\"\"\n",
    "    Permite ao usuário selecionar o modelo de embedding desejado.\n",
    "    \"\"\"\n",
    "    print(\"\\nModelos de embedding disponíveis:\")\n",
    "    for idx, (key, path) in enumerate(EMBEDDING_MODEL_MAP.items(), 1):\n",
    "        print(f\"{idx}. {key}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"\\nEscolha o número do modelo: \"))\n",
    "            if 1 <= choice <= len(EMBEDDING_MODEL_MAP):\n",
    "                return list(EMBEDDING_MODEL_MAP.keys())[choice-1]\n",
    "            print(\"Escolha inválida. Tente novamente.\")\n",
    "        except ValueError:\n",
    "            print(\"Por favor, digite um número válido.\")\n",
    "\n",
    "def rag_query_with_model_selection():\n",
    "    \"\"\"\n",
    "    Interface principal do RAG com seleção de modelo para documentos SEI.\n",
    "    \"\"\"\n",
    "    # 1. Selecionar modelo\n",
    "    model_key = select_embedding_model()\n",
    "\n",
    "    # 2. Carregar modelo e collection\n",
    "    embedding_model, collection = load_embedding_model_and_collection(model_key)\n",
    "    if not embedding_model or not collection:\n",
    "        print(\"Não foi possível carregar o modelo/collection.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 3. Fazer a consulta\n",
    "        query = input(\"\\nDigite sua pergunta: \")\n",
    "\n",
    "        # 4. Executar o RAG\n",
    "        print(\"\\nProcessando pergunta:\", query)\n",
    "        start_time = time.time()\n",
    "\n",
    "        results = retrieve_relevant_chunks(query, embedding_model, collection)\n",
    "        if not results['documents'][0]:\n",
    "            print(\"Nenhum resultado encontrado.\")\n",
    "            return\n",
    "\n",
    "        context = \"\\n\\n\".join(results['documents'][0])\n",
    "        first_metadata = results['metadatas'][0][0] if results['metadatas'][0] else {}\n",
    "\n",
    "        prompt = create_prompt(query, context)\n",
    "        response = generate_response(prompt, tokenizer, llm_model)\n",
    "\n",
    "        # 5. Mostrar resultados\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Chunks recuperados:\")\n",
    "        for doc, meta in zip(results['documents'][0], results['metadatas'][0]):\n",
    "            print(\"\\nConteúdo:\", doc.strip())\n",
    "            print(\"Metadados:\")\n",
    "            for k, v in meta.items():\n",
    "                print(f\"  {k}: {v}\")\n",
    "            print(\"-\"*40)\n",
    "\n",
    "        # Mostrar metadados do primeiro chunk explicitamente\n",
    "        print(\"\\n=== Metadados do documento mais relevante ===\")\n",
    "        for k, v in first_metadata.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "        print(\"\\n=== Resposta do LLM ===\")\n",
    "        print(response)\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Tempo total de processamento: {total_time:.2f} segundos\")\n",
    "\n",
    "    finally:\n",
    "        # 6. Limpar memória\n",
    "        print(\"\\nLiberando memória...\")\n",
    "        del embedding_model\n",
    "        gc.collect()\n",
    "        print(\"Concluído!\")\n",
    "\n",
    "# Executar uma única consulta\n",
    "rag_query_with_model_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e874e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7: Avaliação do pipeline RAG para HTMLs\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def evaluate_rag_pipeline_html():\n",
    "    \"\"\"\n",
    "    Avalia o pipeline RAG para HTMLs usando um conjunto de perguntas predefinidas.\n",
    "    Salva os resultados em um arquivo JSON para posterior avaliação com RAGAS.\n",
    "    \"\"\"\n",
    "    # 1. Carregar o JSON de perguntas (caminho atualizado para HTMLs)\n",
    "    questions_path = \"../RAGAS/03_Questions_HTML/Questions_HTML.json\"\n",
    "    print(f\"Carregando perguntas de {questions_path}...\")\n",
    "\n",
    "    try:\n",
    "        with open(questions_path, 'r', encoding='utf-8') as f:\n",
    "            questions_data = json.load(f)\n",
    "        print(f\"Carregadas {len(questions_data)} perguntas.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar o arquivo de perguntas: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Selecionar o modelo de embedding\n",
    "    print(\"\\nModelos de embedding disponíveis:\")\n",
    "    for idx, (key, path) in enumerate(EMBEDDING_MODEL_MAP.items(), 1):\n",
    "        print(f\"{idx}. {key}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"\\nEscolha o número do modelo: \"))\n",
    "            if 1 <= choice <= len(EMBEDDING_MODEL_MAP):\n",
    "                model_key = list(EMBEDDING_MODEL_MAP.keys())[choice-1]\n",
    "                break\n",
    "            print(\"Escolha inválida. Tente novamente.\")\n",
    "        except ValueError:\n",
    "            print(\"Por favor, digite um número válido.\")\n",
    "\n",
    "    # 3. Carregar modelo e collection\n",
    "    print(f\"\\nCarregando modelo de embedding: {model_key}\")\n",
    "    start_load = time.time()\n",
    "    embedding_model, collection = load_embedding_model_and_collection(model_key)\n",
    "    load_time = time.time() - start_load\n",
    "\n",
    "    if not embedding_model or not collection:\n",
    "        print(\"Não foi possível carregar o modelo/collection.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Modelo e collection carregados em {load_time:.2f} segundos.\")\n",
    "\n",
    "    # 4. Processar cada pergunta e armazenar resultados\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        print(\"\\nProcessando perguntas...\")\n",
    "        for i, question_item in enumerate(tqdm(questions_data)):\n",
    "            question = question_item[\"question\"]\n",
    "            ground_truth = question_item[\"ground_truth\"]\n",
    "\n",
    "            print(f\"\\n\\nPergunta {i+1}/{len(questions_data)}: {question}\")\n",
    "\n",
    "            # Recuperar chunks relevantes usando a função específica para HTMLs\n",
    "            start_time = time.time()\n",
    "            chunks_results = retrieve_relevant_chunks(question, embedding_model, collection)\n",
    "\n",
    "            # Verificar se encontrou resultados\n",
    "            if not chunks_results['documents'][0]:\n",
    "                print(\"Nenhum resultado encontrado.\")\n",
    "                contexts = []\n",
    "                answer = \"Não foi possível encontrar informações relevantes para responder à pergunta.\"\n",
    "            else:\n",
    "                # Preparar contexto e gerar resposta\n",
    "                contexts = chunks_results['documents'][0]\n",
    "                context_text = \"\\n\\n\".join(contexts)\n",
    "                prompt = create_prompt(question, context_text)\n",
    "                answer = generate_response(prompt, tokenizer, llm_model)\n",
    "\n",
    "            process_time = time.time() - start_time\n",
    "\n",
    "            # Exibir resultados\n",
    "            print(\"\\nContextos recuperados:\")\n",
    "            for j, (doc, meta) in enumerate(zip(chunks_results['documents'][0], chunks_results['metadatas'][0])):\n",
    "                print(f\"\\nContexto {j+1}:\")\n",
    "                print(f\"Conteúdo: {doc.strip()}\")\n",
    "                print(\"Metadados:\", meta)\n",
    "\n",
    "            print(\"\\nResposta gerada:\")\n",
    "            print(answer)\n",
    "\n",
    "            print(\"\\nGround Truth:\")\n",
    "            for gt in ground_truth:\n",
    "                print(f\"- {gt}\")\n",
    "\n",
    "            print(f\"\\nTempo de processamento: {process_time:.2f} segundos\")\n",
    "\n",
    "            # Armazenar resultados\n",
    "            result_item = {\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"contexts\": contexts,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"metadata\": {\n",
    "                    \"embedding_model\": model_key,\n",
    "                    \"process_time\": process_time\n",
    "                }\n",
    "            }\n",
    "            results.append(result_item)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro durante o processamento: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # 5. Salvar resultados em um arquivo JSON (caminho atualizado para HTMLs)\n",
    "        # Criar diretório para o modelo se não existir\n",
    "        output_dir = f\"../RAGAS/03_Questions_HTML/{selected_model}/{model_key}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        output_filename = f\"{output_dir}/Answers_{model_key}_HTML.json\"\n",
    "\n",
    "        try:\n",
    "            with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\nResultados salvos em {output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar resultados: {e}\")\n",
    "\n",
    "        # 6. Limpar memória\n",
    "        print(\"\\nLiberando memória...\")\n",
    "        del embedding_model\n",
    "        gc.collect()\n",
    "        print(\"Avaliação concluída!\")\n",
    "\n",
    "# Executar a avaliação\n",
    "evaluate_rag_pipeline_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebdbbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 8: Avaliação com RAGAS 0.2.15 usando OpenAI GPT-4o mini para HTMLs\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Carregar variáveis de ambiente do arquivo .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Obter a chave da API OpenAI do arquivo .env\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    print(\"ERRO: Chave da API OpenAI não encontrada no arquivo .env\")\n",
    "    print(\"Por favor, crie um arquivo .env com o seguinte conteúdo:\")\n",
    "    print(\"OPENAI_API_KEY=sua-chave-api-aqui\")\n",
    "    raise ValueError(\"Chave da API OpenAI não encontrada\")\n",
    "else:\n",
    "    print(\"Chave da API OpenAI carregada com sucesso do arquivo .env\")\n",
    "    # Definir explicitamente a variável de ambiente\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "# Configurar o modelo GPT-4o mini para o RAGAS\n",
    "try:\n",
    "    # Tentar usar LangchainLLM\n",
    "    from ragas.llms import LangchainLLM\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    # Criar instância do modelo OpenAI com a chave do .env\n",
    "    chat_model = ChatOpenAI(\n",
    "        model_name=\"gpt-4o-mini\",  # Usando GPT-4o mini\n",
    "        temperature=0,\n",
    "        openai_api_key=openai_api_key\n",
    "    )\n",
    "\n",
    "    # Configurar RAGAS para usar este modelo\n",
    "    ragas_llm = LangchainLLM(chat_model)\n",
    "    ragas.llms.set_global_llm(ragas_llm)\n",
    "    print(\"RAGAS configurado para usar gpt-4o-mini via LangchainLLM\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao configurar LangchainLLM: {e}\")\n",
    "    print(\"Continuando com o modelo padrão do RAGAS\")\n",
    "\n",
    "# Definir o caminho base para os arquivos RAGAS\n",
    "RAGAS_BASE_PATH = \"../RAGAS\"  # Um nível acima de TCC2/NLP/\n",
    "\n",
    "def load_and_prepare_data(json_path):\n",
    "    \"\"\"\n",
    "    Carrega os dados do JSON e os prepara para avaliação.\n",
    "    \"\"\"\n",
    "    print(f\"Carregando dados de {json_path}...\")\n",
    "\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Preparar dados no formato correto\n",
    "        dataset_dict = {\n",
    "            \"user_input\": [],\n",
    "            \"response\": [],\n",
    "            \"retrieved_contexts\": [],\n",
    "            \"reference\": []\n",
    "        }\n",
    "\n",
    "        for item in data:\n",
    "            dataset_dict[\"user_input\"].append(item[\"question\"])\n",
    "            dataset_dict[\"response\"].append(item[\"answer\"])\n",
    "            dataset_dict[\"retrieved_contexts\"].append(item[\"contexts\"])\n",
    "            dataset_dict[\"reference\"].append(\" \".join(item[\"ground_truth\"]))\n",
    "\n",
    "        # Converter para Dataset do HuggingFace\n",
    "        dataset = Dataset.from_dict(dataset_dict)\n",
    "        print(f\"Dados preparados com {len(dataset)} exemplos.\")\n",
    "        return dataset\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar ou preparar os dados: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_with_ragas(dataset):\n",
    "    \"\"\"\n",
    "    Avalia o dataset usando métricas do RAGAS.\n",
    "    \"\"\"\n",
    "    print(\"Iniciando avaliação com RAGAS...\")\n",
    "\n",
    "    # Definir métricas (adicionando context_precision como quarta métrica)\n",
    "    metrics = [\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_precision\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(\"Usando função evaluate() do RAGAS...\")\n",
    "        result = evaluate(\n",
    "            dataset=dataset,\n",
    "            metrics=metrics\n",
    "        )\n",
    "\n",
    "        print(\"Tipo do resultado:\", type(result))\n",
    "        print(\"Estrutura do resultado:\", dir(result))\n",
    "\n",
    "        # Extrair resultados com base no tipo de objeto retornado\n",
    "        results = {}\n",
    "\n",
    "        # Verificar se o resultado tem um atributo 'scores'\n",
    "        if hasattr(result, 'scores'):\n",
    "            print(\"Resultado tem atributo 'scores'\")\n",
    "\n",
    "            # Verificar se scores é uma lista\n",
    "            if isinstance(result.scores, list):\n",
    "                print(\"scores é uma lista com\", len(result.scores), \"elementos\")\n",
    "\n",
    "                # Imprimir o primeiro elemento para diagnóstico\n",
    "                if result.scores:\n",
    "                    print(\"Primeiro elemento de scores:\", result.scores[0])\n",
    "\n",
    "                    # Tentar extrair métricas do primeiro elemento\n",
    "                    if hasattr(result.scores[0], 'name') and hasattr(result.scores[0], 'score'):\n",
    "                        # Parece ser uma lista de objetos Score\n",
    "                        for score_obj in result.scores:\n",
    "                            metric_name = score_obj.name\n",
    "                            score_value = score_obj.score\n",
    "                            # Corrigir o nome da métrica para começar com 1 em vez de 0\n",
    "                            if metric_name.endswith('_0'):\n",
    "                                metric_name = metric_name[:-1] + '1'\n",
    "                            results[metric_name] = score_value\n",
    "                            print(f\"{metric_name}: {score_value:.4f}\")\n",
    "                    else:\n",
    "                        # Tentar extrair como dicionário\n",
    "                        for i, score in enumerate(result.scores):\n",
    "                            if isinstance(score, dict):\n",
    "                                for metric_name, value in score.items():\n",
    "                                    # Corrigir o índice para começar com 1 em vez de 0\n",
    "                                    results[f\"{metric_name}_{i+1}\"] = value\n",
    "                                    print(f\"{metric_name}_{i+1}: {value:.4f}\")\n",
    "            else:\n",
    "                # Se não for uma lista, tentar como dicionário\n",
    "                try:\n",
    "                    for metric_name, values in result.scores.items():\n",
    "                        # Corrigir o nome da métrica para começar com 1 em vez de 0\n",
    "                        if metric_name.endswith('_0'):\n",
    "                            metric_name = metric_name[:-1] + '1'\n",
    "                        results[metric_name] = values\n",
    "                        avg_score = np.mean(values) if isinstance(values, list) else values\n",
    "                        print(f\"{metric_name}: {avg_score:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao processar scores como dicionário: {e}\")\n",
    "\n",
    "        # Verificar se o resultado tem um método 'to_pandas'\n",
    "        elif hasattr(result, 'to_pandas'):\n",
    "            print(\"Resultado tem método 'to_pandas'\")\n",
    "            df = result.to_pandas()\n",
    "            print(\"Colunas do DataFrame:\", df.columns.tolist())\n",
    "\n",
    "            for col in df.columns:\n",
    "                if col not in dataset.column_names:\n",
    "                    # Corrigir o nome da coluna para começar com 1 em vez de 0\n",
    "                    new_col = col\n",
    "                    if col.endswith('_0'):\n",
    "                        new_col = col[:-1] + '1'\n",
    "                    results[new_col] = df[col].tolist()\n",
    "                    avg_score = np.mean(df[col])\n",
    "                    print(f\"{new_col}: {avg_score:.4f}\")\n",
    "\n",
    "        # Se nenhuma das abordagens acima funcionar, tentar extrair informações básicas\n",
    "        else:\n",
    "            print(\"Formato de resultado desconhecido, tentando extrair informações básicas\")\n",
    "            # Tentar converter para string e extrair informações\n",
    "            result_str = str(result)\n",
    "            print(\"Resultado como string:\", result_str[:500] + \"...\" if len(result_str) > 500 else result_str)\n",
    "\n",
    "        # Verificar se conseguimos extrair algum resultado\n",
    "        if not results:\n",
    "            print(\"Não foi possível extrair resultados automaticamente.\")\n",
    "            print(\"Tentando acessar _scores_dict...\")\n",
    "\n",
    "            if hasattr(result, '_scores_dict'):\n",
    "                try:\n",
    "                    scores_dict = result._scores_dict\n",
    "                    print(\"_scores_dict:\", scores_dict)\n",
    "\n",
    "                    if isinstance(scores_dict, dict):\n",
    "                        for metric_name, values in scores_dict.items():\n",
    "                            # Corrigir o nome da métrica para começar com 1 em vez de 0\n",
    "                            if metric_name.endswith('_0'):\n",
    "                                metric_name = metric_name[:-1] + '1'\n",
    "                            results[metric_name] = values\n",
    "                            avg_score = np.mean(values) if isinstance(values, list) else values\n",
    "                            print(f\"{metric_name}: {avg_score:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao processar _scores_dict: {e}\")\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao avaliar com RAGAS: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {}\n",
    "\n",
    "def visualize_results(results, full_model_name):\n",
    "    \"\"\"\n",
    "    Visualiza os resultados da avaliação e salva na subpasta do embedding.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"Sem resultados para visualizar.\")\n",
    "        return\n",
    "\n",
    "    # Extrair o nome do modelo LLM e do embedding\n",
    "    if \"/\" in full_model_name:\n",
    "        llm_name, embedding_name = full_model_name.split(\"/\", 1)\n",
    "    else:\n",
    "        # Compatibilidade com o formato antigo\n",
    "        llm_name = \"unknown_llm\"\n",
    "        embedding_name = full_model_name\n",
    "\n",
    "    # Título para o gráfico\n",
    "    display_name = f\"{llm_name} + {embedding_name}\"\n",
    "\n",
    "    # Preparar dados para visualização\n",
    "    metrics = []\n",
    "    scores = []\n",
    "\n",
    "    for metric, values in results.items():\n",
    "        if values is not None:\n",
    "            metrics.append(metric)\n",
    "            # Calcular média se for uma lista, caso contrário usar o valor diretamente\n",
    "            if isinstance(values, list):\n",
    "                scores.append(np.mean(values))\n",
    "            else:\n",
    "                scores.append(values)\n",
    "\n",
    "    if not metrics:\n",
    "        print(\"Sem métricas para visualizar.\")\n",
    "        return\n",
    "\n",
    "    # Criar DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Métrica\": metrics,\n",
    "        \"Pontuação\": scores\n",
    "    })\n",
    "\n",
    "    # Plotar gráfico\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=\"Métrica\", y=\"Pontuação\", data=df)\n",
    "    plt.title(f\"Avaliação RAGAS - {display_name}\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Caminho para a subpasta do embedding dentro da pasta do modelo LLM\n",
    "    # Alterado para usar a pasta 03_Questions_HTML\n",
    "    embedding_dir = os.path.join(RAGAS_BASE_PATH, \"03_Questions_HTML\", llm_name, embedding_name)\n",
    "    os.makedirs(embedding_dir, exist_ok=True)\n",
    "\n",
    "    # Salvar figura com o nome correto na subpasta do embedding\n",
    "    output_file = os.path.join(embedding_dir, f\"Answers_{embedding_name}_ragas_results.png\")\n",
    "    plt.savefig(output_file)\n",
    "    plt.show()\n",
    "    print(f\"Gráfico salvo em {output_file}\")\n",
    "\n",
    "    # Salvar resultados em CSV com o nome correto na subpasta do embedding\n",
    "    csv_file = os.path.join(embedding_dir, f\"Answers_{embedding_name}_ragas_results.csv\")\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Resultados salvos em {csv_file}\")\n",
    "\n",
    "def run_ragas_evaluation_html():\n",
    "    \"\"\"\n",
    "    Função principal para executar a avaliação RAGAS para HTMLs.\n",
    "    \"\"\"\n",
    "    # Definir o diretório base de respostas para HTMLs\n",
    "    questions_dir = os.path.join(RAGAS_BASE_PATH, \"03_Questions_HTML\")\n",
    "\n",
    "    # Verificar se o diretório existe\n",
    "    if not os.path.exists(questions_dir):\n",
    "        print(f\"ERRO: Diretório {questions_dir} não encontrado.\")\n",
    "        print(\"Verifique se o caminho está correto.\")\n",
    "        return\n",
    "\n",
    "    # Verificar se a variável selected_model está definida no escopo global\n",
    "    global selected_model\n",
    "\n",
    "    # Verificar se a variável existe e tem um valor\n",
    "    if 'selected_model' not in globals() or not selected_model:\n",
    "        print(\"AVISO: Variável 'selected_model' não encontrada ou vazia.\")\n",
    "        print(\"\\nModelos LLM disponíveis:\")\n",
    "        for idx, model_name in enumerate(AVAILABLE_MODELS.keys(), 1):\n",
    "            print(f\"{idx}. {model_name}\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                model_choice = int(input(\"\\nEscolha o número do modelo LLM para avaliar: \"))\n",
    "                if 1 <= model_choice <= len(AVAILABLE_MODELS):\n",
    "                    selected_model = list(AVAILABLE_MODELS.keys())[model_choice-1]\n",
    "                    break\n",
    "                print(\"Escolha inválida. Tente novamente.\")\n",
    "            except ValueError:\n",
    "                print(\"Por favor, digite um número válido.\")\n",
    "\n",
    "    print(f\"Usando modelo LLM: {selected_model}\")\n",
    "\n",
    "    # Verificar se a pasta do modelo LLM existe\n",
    "    llm_dir = os.path.join(questions_dir, selected_model)\n",
    "    if not os.path.exists(llm_dir):\n",
    "        print(f\"ERRO: Diretório do modelo LLM '{selected_model}' não encontrado em {questions_dir}\")\n",
    "        print(f\"Criando diretório {llm_dir}\")\n",
    "        os.makedirs(llm_dir, exist_ok=True)\n",
    "        print(\"Nenhum embedding encontrado para este modelo.\")\n",
    "        return\n",
    "\n",
    "    # Listar as pastas de embedding dentro do diretório do modelo LLM\n",
    "    embedding_folders = [d for d in os.listdir(llm_dir)\n",
    "                         if os.path.isdir(os.path.join(llm_dir, d))]\n",
    "\n",
    "    if not embedding_folders:\n",
    "        print(f\"Nenhuma pasta de embedding encontrada para o modelo {selected_model}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nModelos de embedding disponíveis:\")\n",
    "    for idx, embedding_name in enumerate(embedding_folders, 1):\n",
    "        print(f\"{idx}. {embedding_name}\")\n",
    "\n",
    "    # Selecionar modelo de embedding para avaliação\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"\\nEscolha o número do modelo de embedding para avaliar (0 para avaliar todos): \"))\n",
    "            if 0 <= choice <= len(embedding_folders):\n",
    "                break\n",
    "            print(\"Escolha inválida. Tente novamente.\")\n",
    "        except ValueError:\n",
    "            print(\"Por favor, digite um número válido.\")\n",
    "\n",
    "    # Avaliar modelos de embedding selecionados\n",
    "    if choice == 0:\n",
    "        embeddings_to_evaluate = embedding_folders\n",
    "    else:\n",
    "        embeddings_to_evaluate = [embedding_folders[choice-1]]\n",
    "\n",
    "    # Executar avaliação para cada modelo de embedding\n",
    "    for embedding_name in embeddings_to_evaluate:\n",
    "        # Caminho para a pasta do embedding\n",
    "        embedding_dir = os.path.join(llm_dir, embedding_name)\n",
    "\n",
    "        # Procurar o arquivo JSON de respostas na pasta do embedding\n",
    "        # Alterado para procurar arquivos com _HTML.json\n",
    "        json_files = [f for f in os.listdir(embedding_dir)\n",
    "                     if f.endswith(\"_HTML.json\")]\n",
    "\n",
    "        if not json_files:\n",
    "            print(f\"Nenhum arquivo JSON encontrado para o embedding {embedding_name}\")\n",
    "            continue\n",
    "\n",
    "        # Usar o primeiro arquivo JSON encontrado\n",
    "        file = json_files[0]\n",
    "        file_path = os.path.join(embedding_dir, file)\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Avaliando {file} para o modelo LLM {selected_model} com embedding {embedding_name}...\")\n",
    "\n",
    "        # Carregar e preparar dados\n",
    "        dataset = load_and_prepare_data(file_path)\n",
    "\n",
    "        if dataset is None:\n",
    "            continue\n",
    "\n",
    "        # Avaliar com RAGAS\n",
    "        results = evaluate_with_ragas(dataset)\n",
    "\n",
    "        # Visualizar resultados - passando o caminho completo incluindo o modelo LLM\n",
    "        full_model_name = f\"{selected_model}/{embedding_name}\"\n",
    "        visualize_results(results, full_model_name)\n",
    "\n",
    "        print(f\"Avaliação de {embedding_name} com modelo LLM {selected_model} concluída!\")\n",
    "\n",
    "# Executar avaliação RAGAS para HTMLs\n",
    "run_ragas_evaluation_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237bf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula: Exibir os 10 chunks mais relevantes\n",
    "\n",
    "# Selecione o modelo de embedding desejado (ajuste se necessário)\n",
    "model_key = \"all-MiniLM-L6-v2\"  # ou outro disponível no seu EMBEDDING_MODEL_MAP\n",
    "\n",
    "# Carregue o modelo de embedding e a coleção correspondente\n",
    "embedding_model, collection = load_embedding_model_and_collection(model_key)\n",
    "if not embedding_model or not collection:\n",
    "    print(\"Não foi possível carregar o modelo/collection.\")\n",
    "else:\n",
    "    consulta = \"Regulamentar as atividades complementares (ACs) dos cursos de graduação\"\n",
    "    print(f\"Buscando chunks relevantes para: '{consulta}'\")\n",
    "    resultados = retrieve_relevant_chunks(consulta, embedding_model, collection, n_results=10)\n",
    "\n",
    "    docs = resultados.get(\"documents\", [[]])[0]\n",
    "    metas = resultados.get(\"metadatas\", [[]])[0]\n",
    "\n",
    "    print(f\"\\nTotal de chunks retornados: {len(docs)}\\n\")\n",
    "    for i, (doc, meta) in enumerate(zip(docs, metas), 1):\n",
    "        print(f\"Chunk {i}:\")\n",
    "        print(\"Conteúdo:\", doc.strip())\n",
    "        print(\"Metadados:\", meta)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Estes chunks foram utilizados para identificar e gerar perguntas e respostas sobre determinados documentos,\n",
    "# permitindo avaliar a qualidade das respostas com o framework RAGAS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
