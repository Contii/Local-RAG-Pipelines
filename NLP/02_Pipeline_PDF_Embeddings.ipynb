{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56f963dc",
   "metadata": {},
   "source": [
    "## Pipeline de Processamento e Armazenamento de Embeddings para Ementas\n",
    "\n",
    "Este pipeline foi desenvolvido para **processar arquivos PDF de ementas de disciplinas**, com o objetivo de **gerar representações vetoriais (embeddings)** e armazená-las em um banco vetorial para consultas futuras.\n",
    "\n",
    "### Ferramentas Principais\n",
    "\n",
    "* **Manipulação de PDFs**: `PyMuPDF` (fitz) para extração de texto.\n",
    "* **Processamento de texto**: Bibliotecas Python padrão (`re`, `os`, etc.) para limpeza e manipulação.\n",
    "* **Geração de Embeddings**: [sentence-transformers](https://www.sbert.net/) para a criação de vetores semânticos.\n",
    "* **Armazenamento Vetorial**: [ChromaDB](https://www.trychroma.com/) para gerenciar e indexar os embeddings.\n",
    "\n",
    "### Estratégia de Chunking e Metadata\n",
    "\n",
    "Cada PDF de ementa é cuidadosamente processado e dividido em **chunks semânticos**, seguindo a estrutura padrão das ementas. Isso inclui seções como:\n",
    "\n",
    "* Identificação (código, nome da disciplina, carga horária)\n",
    "* Objetivo da disciplina\n",
    "* Ementa (resumo do conteúdo)\n",
    "* Conteúdo Programático (dividido por unidades)\n",
    "* Bibliografia Básica\n",
    "* Bibliografia Complementar\n",
    "\n",
    "Cada chunk é enriquecido com **metadados** relevantes, como o código da disciplina, o tipo de seção e a fonte (nome do arquivo PDF), para facilitar futuras buscas e filtragens.\n",
    "\n",
    "### Modelos de Embeddings Utilizados\n",
    "\n",
    "Para cada chunk, embeddings são gerados usando uma seleção de modelos da família SentenceTransformer, garantindo versatilidade e a possibilidade de comparação de desempenho:\n",
    "\n",
    "* `all-MiniLM-L6-v2`\n",
    "* `all-mpnet-base-v2`\n",
    "* `paraphrase-multilingual-MiniLM-L12-v2`\n",
    "* `distiluse-base-multilingual-cased-v2`\n",
    "* `stsb-xlm-r-multilingual`\n",
    "* `neuralmind/bert-base-portuguese-cased`\n",
    "\n",
    "### Armazenamento no ChromaDB\n",
    "\n",
    "Os **embeddings, os textos originais e seus metadados** são armazenados em **coleções separadas no ChromaDB**, uma para cada modelo de embedding. Essa abordagem oferece:\n",
    "\n",
    "* **Comparação de performance**: Facilita a avaliação do desempenho de diferentes modelos em cenários de consulta.\n",
    "* **Flexibilidade na escolha do modelo**: Permite selecionar o modelo mais adequado para cada tipo de consulta.\n",
    "* **Rastreabilidade**: Os metadados garantem a origem e o contexto de cada chunk.\n",
    "\n",
    "### Fluxo do Pipeline\n",
    "\n",
    "1.  **Extração**: Leitura e extração do texto dos PDFs de ementas.\n",
    "2.  **Pré-processamento**: Limpeza e tratamento do texto extraído.\n",
    "3.  **Chunking**: Geração de chunks semânticos baseados na estrutura da ementa.\n",
    "4.  **Embeddings**: Criação de vetores para cada chunk usando todos os modelos definidos.\n",
    "5.  **Armazenamento**: Persistência dos embeddings, textos e metadados nas coleções específicas do ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13167cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1: Importando bibliotecas\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c28f205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2: Configuração dos Modelos de Embeddings\n",
    "def setup_embedding_models() -> List[Tuple[str, SentenceTransformer]]:\n",
    "    models = [\n",
    "        (\"all-MiniLM-L6-v2\", SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')),\n",
    "        (\"all-mpnet-base-v2\", SentenceTransformer('sentence-transformers/all-mpnet-base-v2')),\n",
    "        (\"paraphrase-multilingual-MiniLM-L12-v2\", SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')),\n",
    "        (\"distiluse-base-multilingual-cased-v2\", SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')),\n",
    "        (\"stsb-xlm-r-multilingual\", SentenceTransformer('sentence-transformers/stsb-xlm-r-multilingual')),\n",
    "        (\"neuralmind-bert-base-portuguese-cased\", SentenceTransformer('neuralmind/bert-base-portuguese-cased'))\n",
    "    ]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804575c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3: Configuração do ChromaDB (adaptada para ementas)\n",
    "def setup_chromadb(base_dir: str, models: List[Tuple[str, SentenceTransformer]]) -> Tuple[chromadb.PersistentClient, List[chromadb.Collection]]:\n",
    "    chroma_dir = Path(base_dir) / \"chroma_db\"\n",
    "    chroma_client = chromadb.PersistentClient(path=str(chroma_dir))\n",
    "\n",
    "    collections = []\n",
    "    existing_collections = [col.name for col in chroma_client.list_collections()]\n",
    "\n",
    "    for model_name, _ in models:\n",
    "        collection_name = f\"ementas-disciplinas_{model_name}\"\n",
    "\n",
    "        if collection_name in existing_collections:\n",
    "            print(f\"Excluindo coleção existente: {collection_name}\")\n",
    "            chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "        print(f\"Criando nova coleção: {collection_name}\")\n",
    "        collection = chroma_client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": f\"Ementas de disciplinas usando modelo {model_name}\"}\n",
    "        )\n",
    "        collections.append(collection)\n",
    "\n",
    "    return chroma_client, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4: Funções utilitárias para extração e limpeza\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    text = \"\"\n",
    "    try:\n",
    "        pdf_document = fitz.open(pdf_path)\n",
    "        for page_num in range(pdf_document.page_count):\n",
    "            page = pdf_document.load_page(page_num)\n",
    "            text += page.get_text(\"text\", sort=True)\n",
    "        pdf_document.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair texto do PDF: {e}\")\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "def clean_chunk_text(text: str, chunk_type: str = None) -> str:\n",
    "    if chunk_type == 'identificacao':\n",
    "        # Extrair código da disciplina\n",
    "        codigo_match = re.search(r'(CC|PP|CH|OP|MEIU)\\d+[A-Z0-9]', text)\n",
    "        codigo = codigo_match.group(0) if codigo_match else \"N/A\"\n",
    "\n",
    "        # Extrair nome da disciplina - Método mais robusto\n",
    "        disciplina = \"N/A\"\n",
    "        if codigo_match:\n",
    "            # Tentar diferentes padrões para encontrar o nome da disciplina\n",
    "            patterns = [\n",
    "                # Padrão 1: Após o código até Nota/Conceito\n",
    "                rf'{codigo}\\s+(.*?)(?=Nota/Conceito|Frequência|Presencial|Semestral)',\n",
    "                # Padrão 2: Entre \"Disciplina/Unidade\" e \"Nota/Conceito\"\n",
    "                r'Disciplina/Unidade[^>]*?([^>]*?)(?=Nota/Conceito|Frequência|Presencial|Semestral)',\n",
    "                # Padrão 3: Após \"Curricular\" até \"Nota/Conceito\"\n",
    "                r'Curricular\\s*([^>]*?)(?=Nota/Conceito|Frequência|Presencial|Semestral)'\n",
    "            ]\n",
    "\n",
    "            for pattern in patterns:\n",
    "                disciplina_match = re.search(pattern, text, re.DOTALL)\n",
    "                if disciplina_match:\n",
    "                    disciplina = disciplina_match.group(1).strip()\n",
    "                    # Limpar o nome da disciplina\n",
    "                    disciplina = re.sub(r'\\s+', ' ', disciplina)  # Remove múltiplos espaços\n",
    "                    disciplina = re.sub(r'^\\s*Curricular\\s*', '', disciplina)  # Remove \"Curricular\" do início\n",
    "                    disciplina = disciplina.strip()\n",
    "                    if disciplina and disciplina != \"N/A\":\n",
    "                        break\n",
    "\n",
    "        # Extrair carga horária total\n",
    "        carga_total_match = re.search(r'Total\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+\\s+\\d+\\s+(\\d+)', text)\n",
    "        if not carga_total_match:\n",
    "            # Tentar padrões alternativos\n",
    "            carga_patterns = [\n",
    "                r'Total\\s+(\\d+)\\s+AT:',\n",
    "                r'Total\\s*:\\s*(\\d+)',\n",
    "                r'Total\\s+(\\d+)\\s*h',\n",
    "                r'Total\\s+(\\d+)'\n",
    "            ]\n",
    "            for pattern in carga_patterns:\n",
    "                carga_match = re.search(pattern, text)\n",
    "                if carga_match:\n",
    "                    carga_total_match = carga_match\n",
    "                    break\n",
    "\n",
    "        carga_horaria = carga_total_match.group(1) + \" Horas\" if carga_total_match else \"N/A\"\n",
    "\n",
    "        # Se ainda não encontrou a disciplina, tentar extrair do metadata\n",
    "        if disciplina == \"N/A\" or not disciplina:\n",
    "            disciplina_match = re.search(r'Disciplina:\\s*([^>]*?)(?=\\.|$)', text, re.DOTALL)\n",
    "            if disciplina_match:\n",
    "                disciplina = disciplina_match.group(1).strip()\n",
    "\n",
    "        # Formatar o texto final\n",
    "        text = f\"Informações da disciplina {codigo} {disciplina}. Carga Horária {carga_horaria}\"\n",
    "        return text\n",
    "\n",
    "    # Para os outros chunks, limpeza padrão\n",
    "    lines = text.split('\\n')\n",
    "    lines = [line for line in lines if line.strip() and line.strip().lower() not in [\n",
    "        'ordem', 'ementa', 'conteúdo', 'conteudo', 'resumo da alteração', '#',\n",
    "        'ministério da educação', 'universidade tecnológica federal do paraná', 'campus medianeira',\n",
    "        'informações da disciplina', 'por conteúdo', 'modalidade', 'código', 'disciplina/unidade',\n",
    "        'da oferta', 'ofertado', 'curricular', 'modo de avaliação', 'disciplina', 'at', 'ap', 'aps',\n",
    "        'anp', 'apcc', 'chead', 'che', 'total'\n",
    "    ]]\n",
    "    text = '\\n'.join(lines)\n",
    "    text = re.sub(r'^\\d+\\s+', '', text)\n",
    "    text = re.sub(r'(Bibliografia Complementar\\s*)+$', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_section_from_pdf(text: str, section_start: str, section_end: str = None) -> str:\n",
    "    if section_end:\n",
    "        pattern = f\"{section_start}(.*?){section_end}\"\n",
    "    else:\n",
    "        pattern = f\"{section_start}(.*?)(?=(?:Bibliografia|Resumo da Alteração|$))\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "def extract_metadata_from_header(text: str) -> dict:\n",
    "    metadata = {}\n",
    "    # Extrair código e nome da disciplina de forma mais robusta\n",
    "    codigo_match = re.search(r'(CC|PP)\\d+[A-Z]', text)\n",
    "    if codigo_match:\n",
    "        metadata['codigo'] = codigo_match.group(0)\n",
    "        # Tentar encontrar o nome da disciplina após o código\n",
    "        disciplina_text = text[codigo_match.end():].split('\\n')[0]\n",
    "        metadata['disciplina'] = disciplina_text.strip()\n",
    "    # Extrair modalidade e oferta\n",
    "    metadata['modalidade'] = 'Presencial' if 'Presencial' in text else 'Não Presencial'\n",
    "    metadata['oferta'] = 'Semestral' if 'Semestral' in text else 'Anual'\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a789e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5: Processamento dos PDFs e chunking estruturado\n",
    "\n",
    "def process_pdf_content(text: str, filename: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Processa o conteúdo do PDF e retorna metadados e chunks.\n",
    "    \"\"\"\n",
    "    metadata = extract_metadata_from_header(text)\n",
    "\n",
    "    # Extrair código do nome do arquivo se não foi encontrado no texto\n",
    "    if not metadata.get('codigo'):\n",
    "        metadata['codigo'] = os.path.splitext(filename)[0]  # Remove a extensão .pdf\n",
    "\n",
    "    # Tentar extrair disciplina do texto usando o código\n",
    "    if not metadata.get('disciplina'):\n",
    "        disciplina_pattern = f\"{metadata['codigo']}(.*?)(?=Frequência|Presencial|Nota|$)\"\n",
    "        disciplina_match = re.search(disciplina_pattern, text, re.DOTALL)\n",
    "        if disciplina_match:\n",
    "            disciplina = disciplina_match.group(1).strip()\n",
    "            metadata['disciplina'] = disciplina\n",
    "        else:\n",
    "            # Se ainda não encontrou, procurar após \"Disciplina\" ou \"Unidade Curricular\"\n",
    "            disc_match = re.search(r'(?:Disciplina|Unidade Curricular)[:\\s]+(.*?)(?=Frequência|Presencial|Nota|$)', text, re.DOTALL)\n",
    "            if disc_match:\n",
    "                metadata['disciplina'] = disc_match.group(1).strip()\n",
    "            else:\n",
    "                metadata['disciplina'] = f\"Disciplina {metadata['codigo']}\"\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    # Chunk de identificação\n",
    "    chunks.append({\n",
    "        'text': clean_chunk_text(text[:500], chunk_type='identificacao'),\n",
    "        'tipo': 'identificacao',\n",
    "        'metadata': metadata.copy()\n",
    "    })\n",
    "\n",
    "    # Chunk de objetivo\n",
    "    objetivo = extract_section_from_pdf(text, \"Objetivo\", \"Ementa\")\n",
    "    if objetivo:\n",
    "        chunks.append({\n",
    "            'text': clean_chunk_text(objetivo),\n",
    "            'tipo': 'objetivo',\n",
    "            'metadata': metadata.copy()\n",
    "        })\n",
    "\n",
    "    # Chunk de ementa\n",
    "    ementa = extract_section_from_pdf(text, \"Ementa\", \"Conteúdo Programático\")\n",
    "    if ementa:\n",
    "        chunks.append({\n",
    "            'text': clean_chunk_text(ementa),\n",
    "            'tipo': 'ementa',\n",
    "            'metadata': metadata.copy()\n",
    "        })\n",
    "\n",
    "    # Chunks de conteúdo programático\n",
    "    conteudo = extract_section_from_pdf(text, \"Conteúdo Programático\", \"Bibliografia\")\n",
    "    if conteudo:\n",
    "        topicos = re.findall(r'(\\d+\\s*.*?(?=\\d+\\s*|Bibliografia|$))', conteudo, re.DOTALL)\n",
    "        for topico in topicos:\n",
    "            chunks.append({\n",
    "                'text': clean_chunk_text(topico.strip()),\n",
    "                'tipo': 'conteudo_programatico',\n",
    "                'metadata': {**metadata.copy(), 'secao': 'Tópico ' + topico.split()[0]}\n",
    "            })\n",
    "\n",
    "    # Chunks de bibliografia\n",
    "    bibliografia_basica = extract_section_from_pdf(text, \"Bibliografia Básica\", \"Bibliografia Complementar\")\n",
    "    if bibliografia_basica:\n",
    "        chunks.append({\n",
    "            'text': clean_chunk_text(bibliografia_basica),\n",
    "            'tipo': 'bibliografia_basica',\n",
    "            'metadata': metadata.copy()\n",
    "        })\n",
    "\n",
    "    bibliografia_complementar = extract_section_from_pdf(text, \"Bibliografia Complementar\", \"Resumo\")\n",
    "    if bibliografia_complementar:\n",
    "        chunks.append({\n",
    "            'text': clean_chunk_text(bibliografia_complementar),\n",
    "            'tipo': 'bibliografia_complementar',\n",
    "            'metadata': metadata.copy()\n",
    "        })\n",
    "\n",
    "    return metadata, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "970d1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6: Processamento e inserção em múltiplas coleções\n",
    "def process_pdf_and_add_to_collections(\n",
    "    pdf_path: str,\n",
    "    models: List[Tuple[str, SentenceTransformer]],\n",
    "    collections: List[chromadb.Collection]\n",
    "):\n",
    "    \"\"\"\n",
    "    Processa um único PDF e adiciona seus chunks em todas as coleções.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extrair nome do arquivo e texto\n",
    "        source = os.path.basename(pdf_path)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "        if not text:\n",
    "            print(f\"Não foi possível extrair texto de {pdf_path}. Pulando...\")\n",
    "            return\n",
    "\n",
    "        # Processar conteúdo e gerar chunks\n",
    "        metadata, chunks = process_pdf_content(text, source)\n",
    "\n",
    "        # Adicionar informações de fonte\n",
    "        metadata['source'] = source\n",
    "        metadata['codigo'] = os.path.splitext(source)[0]\n",
    "\n",
    "        print(f\"\\nProcessando: {source}\")\n",
    "        print(f\"Disciplina: {metadata.get('disciplina', 'N/A')}\")\n",
    "        print(f\"Código: {metadata['codigo']}\")\n",
    "\n",
    "        # Para cada modelo/coleção, gerar embeddings e inserir\n",
    "        for (model_name, embedding_model), collection in zip(models, collections):\n",
    "            print(f\"  Inserindo na coleção: {collection.name}\")\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_text = chunk['text']\n",
    "                chunk_metadata = {\n",
    "                    **chunk['metadata'],\n",
    "                    'source': source,\n",
    "                    'tipo': chunk['tipo'],\n",
    "                    'codigo': metadata['codigo']\n",
    "                }\n",
    "\n",
    "                # Gerar embedding específico para este modelo\n",
    "                embedding = embedding_model.encode(chunk_text).tolist()\n",
    "\n",
    "                # ID único para este chunk nesta coleção\n",
    "                chunk_id = f\"{metadata['codigo']}_{chunk['tipo']}_{i}_{model_name}\"\n",
    "\n",
    "                # Adicionar à coleção\n",
    "                collection.add(\n",
    "                    embeddings=[embedding],\n",
    "                    documents=[chunk_text],\n",
    "                    metadatas=[chunk_metadata],\n",
    "                    ids=[chunk_id]\n",
    "                )\n",
    "\n",
    "            print(f\"    {len(chunks)} chunks adicionados.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar {pdf_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_all_pdfs_in_directory(\n",
    "    pdf_dir: str,\n",
    "    models: List[Tuple[str, SentenceTransformer]],\n",
    "    collections: List[chromadb.Collection]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Processa todos os PDFs em um diretório e adiciona em todas as coleções.\n",
    "    Retorna um dicionário com os chunks organizados por disciplina.\n",
    "    \"\"\"\n",
    "    # Verificar e listar arquivos PDF\n",
    "    if not os.path.exists(pdf_dir):\n",
    "        print(f\"Diretório não encontrado: {pdf_dir}\")\n",
    "        return {}\n",
    "\n",
    "    pdf_files = [os.path.join(pdf_dir, f) for f in os.listdir(pdf_dir)\n",
    "                 if f.lower().endswith('.pdf')]\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"Nenhum arquivo PDF encontrado.\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"Encontrados {len(pdf_files)} arquivos PDF em {pdf_dir}\")\n",
    "\n",
    "    # Dicionário para armazenar chunks por disciplina\n",
    "    chunks_por_disciplina = {}\n",
    "\n",
    "    # Processar cada PDF\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            # Extrair texto e processar chunks\n",
    "            source = os.path.basename(pdf_file)\n",
    "            text = extract_text_from_pdf(pdf_file)\n",
    "\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            # Processar conteúdo e gerar chunks\n",
    "            metadata, chunks = process_pdf_content(text, source)\n",
    "            codigo = metadata['codigo']\n",
    "\n",
    "            # Armazenar chunks no dicionário\n",
    "            chunks_por_disciplina[codigo] = chunks\n",
    "\n",
    "            print(f\"\\nProcessando: {source}\")\n",
    "            print(f\"Disciplina: {metadata.get('disciplina', 'N/A')}\")\n",
    "            print(f\"Código: {codigo}\")\n",
    "\n",
    "            # Adicionar em todas as coleções\n",
    "            for (model_name, embedding_model), collection in zip(models, collections):\n",
    "                print(f\"  Inserindo na coleção: {collection.name}\")\n",
    "\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    chunk_text = chunk['text']\n",
    "                    chunk_metadata = {\n",
    "                        **chunk['metadata'],\n",
    "                        'source': source,\n",
    "                        'tipo': chunk['tipo'],\n",
    "                        'codigo': codigo\n",
    "                    }\n",
    "\n",
    "                    embedding = embedding_model.encode(chunk_text).tolist()\n",
    "                    chunk_id = f\"{codigo}_{chunk['tipo']}_{i}_{model_name}\"\n",
    "\n",
    "                    collection.add(\n",
    "                        embeddings=[embedding],\n",
    "                        documents=[chunk_text],\n",
    "                        metadatas=[chunk_metadata],\n",
    "                        ids=[chunk_id]\n",
    "                    )\n",
    "\n",
    "                print(f\"    {len(chunks)} chunks adicionados.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {pdf_file}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    print(\"\\nProcessamento concluído!\")\n",
    "    return chunks_por_disciplina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170aef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7: Execução do Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Configurar modelos de embedding\n",
    "    models = setup_embedding_models()\n",
    "\n",
    "    # Configurar ChromaDB e coleções\n",
    "    base_dir = os.getcwd()\n",
    "    chroma_client, collections = setup_chromadb(base_dir, models)\n",
    "\n",
    "    # Configurar diretório dos PDFs\n",
    "    pdf_dir = os.path.normpath(os.path.join(\n",
    "        base_dir, \"..\", \"Webscraping\", \"Files\", \"PDF\", \"Ementas\", \"Ciência da Computação\"\n",
    "    ))\n",
    "\n",
    "    # Processar todos os arquivos PDF\n",
    "    chunks_por_disciplina = process_all_pdfs_in_directory(pdf_dir, models, collections)\n",
    "\n",
    "    # Imprimir resumo do processamento\n",
    "    print(\"\\nResumo do processamento:\")\n",
    "    print(f\"Total de disciplinas processadas: {len(chunks_por_disciplina) if chunks_por_disciplina else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ee2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula de Visualização: Exibe chunks principais de todas as disciplinas\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# (Reutilize as funções extract_text_from_pdf, clean_chunk_text, extract_section_from_pdf, extract_metadata_from_header, process_pdf_content da Célula 4)\n",
    "\n",
    "def display_chunks_overview(pdf_dir: str):\n",
    "    \"\"\"\n",
    "    Exibe os chunks de identificação, objetivo e ementa de cada PDF no diretório.\n",
    "    \"\"\"\n",
    "    # Listar arquivos PDF\n",
    "    pdf_files = [os.path.join(pdf_dir, f) for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')]\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(\"Nenhum arquivo PDF encontrado.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Encontrados {len(pdf_files)} arquivos PDF em {pdf_dir}\")\n",
    "\n",
    "    # Processar cada PDF\n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            # Extrair nome do arquivo\n",
    "            source = os.path.basename(pdf_file)\n",
    "\n",
    "            # Extrair texto do PDF\n",
    "            text = extract_text_from_pdf(pdf_file)\n",
    "            if not text:\n",
    "                print(f\"Não foi possível extrair texto de {pdf_file}. Pulando...\")\n",
    "                continue\n",
    "\n",
    "            # Processar conteúdo e gerar chunks\n",
    "            metadata, chunks = process_pdf_content(text, source)\n",
    "\n",
    "            print(f\"\\nArquivo: {source}\")\n",
    "            print(f\"Disciplina: {metadata.get('disciplina', 'N/A')}\")\n",
    "            print(f\"Código: {metadata['codigo']}\")\n",
    "            print(\"=\" * 40)\n",
    "\n",
    "            # Exibir chunks principais\n",
    "            for chunk in chunks:\n",
    "                if chunk['tipo'] in ['identificacao', 'objetivo', 'ementa']:\n",
    "                    print(f\"\\nTipo: {chunk['tipo']}\")\n",
    "                    print(f\"Texto: {chunk['text']}\")\n",
    "                    print(\"-\" * 20)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {pdf_file}: {str(e)}\")\n",
    "\n",
    "# Exemplo de uso:\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = os.getcwd()\n",
    "    pdf_dir = os.path.normpath(os.path.join(\n",
    "        base_dir, \"..\", \"Webscraping\", \"Files\", \"PDF\", \"Ementas\", \"Ciência da Computação\"\n",
    "    ))\n",
    "    display_chunks_overview(pdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cb97efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo: OP63P.pdf\n",
      "Disciplina: Aspectos Formais Da\n",
      "Computação\n",
      "Código: CC56E\n",
      "============================================================\n",
      "\n",
      "Chunk 1:\n",
      "Tipo: identificacao\n",
      "Texto: Informações da disciplina CC56E Aspectos Formais Da Computação. Carga Horária 45 Horas\n",
      "Metadados: {'codigo': 'CC56E', 'disciplina': 'Aspectos Formais Da\\nComputação', 'modalidade': 'Presencial', 'oferta': 'Semestral'}\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 2:\n",
      "Tipo: objetivo\n",
      "Texto: Dar ao aluno noção formal de algoritmo, complexidade e computabilidade e do problema de decisão, de modo a deixá-lo consciente das limitações da ciência da computação. Aparelhá-lo com as ferramentas de modo a habilitá-lo a melhor solucionar problemas com o auxílio do computador e técnicas de computação.\n",
      "Metadados: {'codigo': 'CC56E', 'disciplina': 'Aspectos Formais Da\\nComputação', 'modalidade': 'Presencial', 'oferta': 'Semestral'}\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 3:\n",
      "Tipo: ementa\n",
      "Texto: Problemas solucionáveis e não solucionáveis. Aplicação da teoria dos grafos. Complexidade Computacional. Modelos equivalentes a máquina de Turing: autômato com duas pilhas, máquina de Post, máquina de registradores. Classes de problemas.\n",
      "Metadados: {'codigo': 'CC56E', 'disciplina': 'Aspectos Formais Da\\nComputação', 'modalidade': 'Presencial', 'oferta': 'Semestral'}\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 4:\n",
      "Tipo: conteudo_programatico\n",
      "Texto: Complexidade Computacional. algoritmos. Complexidade no melhor caso. Complexidade de caso médio. Complexidade no pior caso.\n",
      "Metadados: {'codigo': 'CC56E', 'disciplina': 'Aspectos Formais Da\\nComputação', 'modalidade': 'Presencial', 'oferta': 'Semestral', 'secao': 'Tópico 1'}\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 5:\n",
      "Tipo: conteudo_programatico\n",
      "Texto: Modelos equivalentes à máquina de Turing. Revisão das Máquinas de Turing Universais e suas generalizações. Autômato com duas pilhas. Máquina de Post Máquina de registradores.\n",
      "Metadados: {'codigo': 'CC56E', 'disciplina': 'Aspectos Formais Da\\nComputação', 'modalidade': 'Presencial', 'oferta': 'Semestral', 'secao': 'Tópico 2'}\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 6:\n",
      "Tipo: conteudo_programatico\n",
      "Texto: Classes de problemas. Classe P. Classe NP. Classe NP- completo. Tese de Church. Decidibilidade.\n",
      "Metadados: {'codigo': 'CC56E', 'disciplina': 'Aspectos Formais Da\\nComputação', 'modalidade': 'Presencial', 'oferta': 'Semestral', 'secao': 'Tópico 3'}\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 7:\n",
      "Tipo: conteudo_programatico\n",
      "Texto: Problemas solucionáveis e não solucionáveis. Redutibilidade. Intratabilidade.\n",
      "Metadados: {'codigo': 'CC56E', 'disciplina': 'Aspectos Formais Da\\nComputação', 'modalidade': 'Presencial', 'oferta': 'Semestral', 'secao': 'Tópico 4'}\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 8:\n",
      "Tipo: conteudo_programatico\n",
      "Texto: Aplicação da teoria dos grafos. Solução de problemas computacionais por meio de grafos.\n",
      "Metadados: {'codigo': 'CC56E', 'disciplina': 'Aspectos Formais Da\\nComputação', 'modalidade': 'Presencial', 'oferta': 'Semestral', 'secao': 'Tópico 5'}\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 9:\n",
      "Tipo: bibliografia_basica\n",
      "Texto: ROSA, João Luís Garcia. Linguagens formais e autômatos. Rio de Janeiro, RJ: LTC, 2010. 146 p. ISBN 9788521617518. DAVIS, Martin D.; SIGAL, Ron; WEYUKER, Elaine J. Computability, complexity and languages: fundamentals of theoretical computer science . 2. ed. San Diego: Morgan Kaufmann, 1994. 609 p. (Computer sience and scientific computing). ISBN 0-12- 206382-1. HOPCROFT, John E.; ULLMAN, Jeffrey D.; MOTWANI, Rajeev. Introdução à teoria de autômatos, linguagens e computação. Rio de Janeiro, RJ: Campus, c2003. 560 p. ISBN 8535210725.\n",
      "Metadados: {'codigo': 'CC56E', 'disciplina': 'Aspectos Formais Da\\nComputação', 'modalidade': 'Presencial', 'oferta': 'Semestral'}\n",
      "----------------------------------------\n",
      "\n",
      "Chunk 10:\n",
      "Tipo: bibliografia_complementar\n",
      "Texto: VIEIRA, Newton José. Introdução aos fundamentos da computação: linguagens e máquinas. São Paulo: Thomson, 2006. xiii, 319 p. ISBN 8522105081. MENEZES, Paulo Blauth. Matemática discreta para computação e informática. 4. ed. Porto Alegre, RS: Bookman, 2013. 348 p. (Livros didáticos (Universidade Federal do Rio Grande do Sul. Instituto de Informática) ; n. 16). ISBN 9788582600245. TOSCANI, Laira Vieira; VELOSO, Paulo A. S. Complexidade de algoritmos: análise, projeto e métodos. 3. ed. Porto Alegre, RS: Bookman, 2012. 262 p. (Livros didáticos informática UFRGS ; 13). ISBN 9788540701380. SIPSER, Michael. Introdução à teoria da computação. São Paulo, SP: Thomson Learning, c2006. xxi, 459 p. ISBN 9788522104994. MENEZES, Paulo Blauth. Linguagens formais e autômatos. 6. ed. Porto Alegre, RS: Bookman, 2011. 256 p. (Livros didáticos (Universidade Federal do Rio Grande do Sul. Instituto de Informática) ; 3). ISBN 9788577807659.\n",
      "Metadados: {'codigo': 'CC56E', 'disciplina': 'Aspectos Formais Da\\nComputação', 'modalidade': 'Presencial', 'oferta': 'Semestral'}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Célula: Visualizar todos os chunks e metadados da disciplina do documento OP63P.pdf\n",
    "\n",
    "# Defina o caminho do PDF\n",
    "base_dir = os.getcwd()\n",
    "pdf_path = os.path.normpath(os.path.join(\n",
    "    base_dir, \"..\", \"Webscraping\", \"Files\", \"PDF\", \"Ementas\", \"Ciência da Computação\", \"CC56E.pdf\"\n",
    "))\n",
    "\n",
    "# Extraia o texto do PDF\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "if not text:\n",
    "    print(\"Não foi possível extrair texto do PDF.\")\n",
    "else:\n",
    "    # Gere os chunks e metadados\n",
    "    metadata, chunks = process_pdf_content(text, \"OP63P.pdf\")\n",
    "    print(f\"Arquivo: OP63P.pdf\")\n",
    "    print(f\"Disciplina: {metadata.get('disciplina', 'N/A')}\")\n",
    "    print(f\"Código: {metadata.get('codigo', 'N/A')}\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"\\nChunk {i+1}:\")\n",
    "        print(f\"Tipo: {chunk['tipo']}\")\n",
    "        print(f\"Texto: {chunk['text']}\")\n",
    "        print(f\"Metadados: {chunk['metadata']}\")\n",
    "        print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
