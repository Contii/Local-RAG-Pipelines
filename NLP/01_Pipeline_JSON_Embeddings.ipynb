{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507ddd64",
   "metadata": {},
   "source": [
    "## Pipeline de Processamento e Armazenamento de Embeddings\n",
    "\n",
    "Este pipeline foi projetado para **extrair, limpar e converter informações acadêmicas** de arquivos JSON em **representações vetoriais (embeddings)**, que são então armazenadas para futuras consultas semânticas. O foco é preparar uma base de dados vetorial robusta para sistemas de busca e aplicações de Recuperação Aumentada por Geração (RAG).\n",
    "\n",
    "### Ferramentas Principais\n",
    "\n",
    "* **Manipulação de arquivos e dados**: Bibliotecas Python padrão (`os`, `json`, `pathlib`, etc.).\n",
    "* **Geração de Embeddings**: [sentence-transformers](https://www.sbert.net/) para a criação de vetores semânticos.\n",
    "* **Armazenamento Vetorial**: [ChromaDB](https://www.trychroma.com/) para gerenciar e indexar os embeddings.\n",
    "\n",
    "### Estratégia de Chunking e Metadata\n",
    "\n",
    "Cada arquivo JSON é segmentado em **chunks semânticos**, garantindo que as informações relevantes sejam agrupadas de forma coesa. Alguns exemplos de agrupamentos incluem:\n",
    "\n",
    "* Dados básicos do professor (nome, departamento, período).\n",
    "* Disciplinas ministradas (nome, código, turma, formato).\n",
    "* Horários detalhados por disciplina e dia da semana.\n",
    "\n",
    "Cada chunk é enriquecido com **metadados** que facilitam buscas e filtragens futuras, como o nome do professor ou o código da disciplina.\n",
    "\n",
    "### Modelos de Embeddings Utilizados\n",
    "\n",
    "Para cada chunk, embeddings são gerados usando uma variedade de modelos da família SentenceTransformer, permitindo flexibilidade e comparação de desempenho:\n",
    "\n",
    "* `all-MiniLM-L6-v2`\n",
    "* `all-mpnet-base-v2`\n",
    "* `paraphrase-multilingual-MiniLM-L12-v2`\n",
    "* `distiluse-base-multilingual-cased-v2`\n",
    "* `stsb-xlm-r-multilingual`\n",
    "* `bert-large-portuguese-cased` (neuralmind)\n",
    "\n",
    "### Armazenamento no ChromaDB\n",
    "\n",
    "Os **embeddings, textos originais e seus metadados** são organizados e armazenados em coleções separadas no ChromaDB. Cada modelo de embedding possui sua própria coleção, o que permite avaliar e comparar a performance de diferentes modelos em consultas subsequentes.\n",
    "\n",
    "### Fluxo do Pipeline\n",
    "\n",
    "1.  **Leitura**: Ingestão de arquivos JSON de professores.\n",
    "2.  **Chunking**: Divisão dos dados em unidades semânticas com metadados.\n",
    "3.  **Embeddings**: Geração de vetores para cada chunk usando todos os modelos definidos.\n",
    "4.  **Armazenamento**: Persistência dos embeddings, textos e metadados nas respectivas coleções do ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ee5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1: Importando bibliotecas\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a25134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2: Configuração dos Modelos de Embeddings\n",
    "def setup_embedding_models() -> List[Tuple[str, SentenceTransformer]]:\n",
    "    models = [\n",
    "        (\"all-MiniLM-L6-v2\", SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')),\n",
    "        (\"all-mpnet-base-v2\", SentenceTransformer('sentence-transformers/all-mpnet-base-v2')),\n",
    "        (\"paraphrase-multilingual-MiniLM-L12-v2\", SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')),\n",
    "        (\"distiluse-base-multilingual-cased-v2\", SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')),\n",
    "        (\"stsb-xlm-r-multilingual\", SentenceTransformer('sentence-transformers/stsb-xlm-r-multilingual')),\n",
    "        (\"neuralmind-bert-base-portuguese-cased\", SentenceTransformer('neuralmind/bert-base-portuguese-cased'))\n",
    "    ]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6771f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3: Configuração do ChromaDB\n",
    "def setup_chromadb(base_dir: str, models: List[Tuple[str, SentenceTransformer]]) -> Tuple[chromadb.PersistentClient, List[chromadb.Collection]]:\n",
    "    chroma_dir = Path(base_dir) / \"chroma_db\"\n",
    "    chroma_client = chromadb.PersistentClient(path=str(chroma_dir))\n",
    "\n",
    "    collections = []\n",
    "    # Listar todas as coleções existentes uma única vez\n",
    "    existing_collections = [col.name for col in chroma_client.list_collections()]\n",
    "\n",
    "    for model_name, _ in models:\n",
    "        collection_name = f\"horarios-professores_{model_name}\"\n",
    "\n",
    "        # Deletar a coleção se existir\n",
    "        if collection_name in existing_collections:\n",
    "            print(f\"Excluindo coleção existente: {collection_name}\")\n",
    "            chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "        # Criar nova coleção\n",
    "        print(f\"Criando nova coleção: {collection_name}\")\n",
    "        collection = chroma_client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": f\"Horários e informações usando modelo {model_name}\"}\n",
    "        )\n",
    "        collections.append(collection)\n",
    "\n",
    "    return chroma_client, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b7e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4: Função de Chunking Semântico\n",
    "def create_text_chunks_semantico(json_data: Dict) -> List[Dict]:\n",
    "    chunks = []\n",
    "    professor = json_data['nome_professor']\n",
    "    departamento = json_data['departamento']\n",
    "    periodo = json_data['ano_periodo']\n",
    "\n",
    "    # Chunk com informações básicas do professor\n",
    "    chunks.append({\n",
    "        'type': 'info_professor',\n",
    "        'text': f\"O professor {professor} pertence ao departamento {departamento} e está ativo no período {periodo}.\",\n",
    "        'metadata': {\n",
    "            'professor': professor,\n",
    "            'departamento': departamento,\n",
    "            'periodo': periodo\n",
    "        }\n",
    "    })\n",
    "\n",
    "    # Criar um dicionário para agrupar horários por disciplina\n",
    "    disciplinas_horarios = {}\n",
    "\n",
    "    # Coletar todos os horários por código de disciplina\n",
    "    for dia, periodos in json_data['horarios'].items():\n",
    "        for periodo_nome, horarios_dia in periodos.items():\n",
    "            for horario_nome, info in horarios_dia.items():\n",
    "                if info['codigo']:  # Se existe um código de disciplina\n",
    "                    codigo = info['codigo']\n",
    "                    if codigo not in disciplinas_horarios:\n",
    "                        disciplinas_horarios[codigo] = {}\n",
    "\n",
    "                    if dia not in disciplinas_horarios[codigo]:\n",
    "                        disciplinas_horarios[codigo][dia] = []\n",
    "\n",
    "                    disciplinas_horarios[codigo][dia].append({\n",
    "                        'horario': horario_nome,\n",
    "                        'inicio': info['inicio'],\n",
    "                        'fim': info['fim'],\n",
    "                        'sala': info['sala']\n",
    "                    })\n",
    "\n",
    "    # Para cada disciplina no JSON, criar um chunk organizado\n",
    "    for codigo, disciplinas in json_data['disciplinas'].items():\n",
    "        for disc in disciplinas:\n",
    "            nome_disciplina = disc['nome']\n",
    "            turma = disc['turma']\n",
    "            enquadramento = disc['enquadramento']\n",
    "\n",
    "            # Texto base da disciplina\n",
    "            texto_disciplina = (\n",
    "                f\"O professor {professor} ministra a disciplina {nome_disciplina} ({codigo}) \"\n",
    "                f\"para a turma {turma} no formato {enquadramento}.\"\n",
    "            )\n",
    "\n",
    "            # Adicionar horários organizados por dia\n",
    "            if codigo in disciplinas_horarios:\n",
    "                texto_horarios = \"\\nHorários:\"\n",
    "                for dia, horarios in sorted(disciplinas_horarios[codigo].items()):\n",
    "                    # Ordenar horários pelo nome (m1, m2, etc.)\n",
    "                    horarios_ordenados = sorted(horarios, key=lambda x: x['horario'])\n",
    "\n",
    "                    # Formatar horários do dia\n",
    "                    horarios_texto = [\n",
    "                        f\"{h['horario']} ({h['inicio']}-{h['fim']})\"\n",
    "                        for h in horarios_ordenados\n",
    "                    ]\n",
    "\n",
    "                    # Adicionar sala se disponível\n",
    "                    sala = next((h['sala'] for h in horarios_ordenados if h['sala']), \"\")\n",
    "                    sala_texto = f\" - Sala {sala}\" if sala else \"\"\n",
    "\n",
    "                    texto_horarios += f\"\\n- {dia.capitalize()}: {', '.join(horarios_texto)}{sala_texto}\"\n",
    "\n",
    "            else:\n",
    "                texto_horarios = \"\"\n",
    "\n",
    "            # Criar o chunk completo\n",
    "            chunks.append({\n",
    "                'type': 'disciplina',\n",
    "                'text': texto_disciplina + texto_horarios,\n",
    "                'metadata': {\n",
    "                    'professor': professor,\n",
    "                    'departamento': departamento,\n",
    "                    'periodo': periodo,\n",
    "                    'codigo': codigo,\n",
    "                    'nome_disciplina': nome_disciplina,\n",
    "                    'turma': turma\n",
    "                }\n",
    "            })\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3024e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5: Processamento dos Arquivos JSON\n",
    "def process_json_files(base_dir: str, chroma_client: chromadb.PersistentClient,\n",
    "                      models: List[Tuple[str, SentenceTransformer]],\n",
    "                      collections: List[chromadb.Collection]):\n",
    "    json_dir = os.path.normpath(os.path.join(base_dir, \"..\", \"Webscraping\", \"Files\", \"JSON\", \"2025-1\"))\n",
    "\n",
    "    if not os.path.exists(json_dir):\n",
    "        print(f\"Erro: Diretório base '{json_dir}' não encontrado.\")\n",
    "        return {}\n",
    "\n",
    "    chunks_por_professor = {}\n",
    "    id_counter = 0\n",
    "\n",
    "    # Coletar todos os chunks\n",
    "    all_chunks = []\n",
    "    for dept_name in os.listdir(json_dir):\n",
    "        dept_dir = os.path.join(json_dir, dept_name)\n",
    "        if os.path.isdir(dept_dir):\n",
    "            print(f\"Processando departamento: {dept_name}\")\n",
    "\n",
    "            for file_name in os.listdir(dept_dir):\n",
    "                if file_name.lower().endswith(\".json\"):\n",
    "                    json_file = os.path.join(dept_dir, file_name)\n",
    "                    try:\n",
    "                        print(f\"Lendo arquivo JSON: {json_file}\")\n",
    "                        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                            data = json.load(f)\n",
    "\n",
    "                        professor = data['nome_professor']\n",
    "                        print(f\"Processando professor: {professor}\")\n",
    "\n",
    "                        chunks = create_text_chunks_semantico(data)\n",
    "                        print(f\"Criados {len(chunks)} chunks para {professor}\")\n",
    "                        all_chunks.extend(chunks)\n",
    "                        chunks_por_professor[professor] = chunks\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erro ao processar {file_name}: {str(e)}\")\n",
    "\n",
    "    # Processar chunks em lotes\n",
    "    batch_size = 100  # Ajuste conforme necessário\n",
    "\n",
    "    # Criar embeddings com cada modelo e adicionar às respectivas coleções\n",
    "    print(\"\\nGerando embeddings e adicionando às coleções...\")\n",
    "    for (model_name, model), collection in zip(models, collections):\n",
    "        print(f\"\\nProcessando modelo: {model_name}\")\n",
    "\n",
    "        # Processar em lotes\n",
    "        for i in range(0, len(all_chunks), batch_size):\n",
    "            batch_chunks = all_chunks[i:i + batch_size]\n",
    "\n",
    "            try:\n",
    "                # Preparar lotes de dados\n",
    "                texts = [chunk['text'] for chunk in batch_chunks]\n",
    "                metadatas = [chunk['metadata'] for chunk in batch_chunks]\n",
    "                ids = [f\"doc_{j}\" for j in range(id_counter, id_counter + len(batch_chunks))]\n",
    "\n",
    "                # *** FILTRAR TEXTOS VAZIOS ***\n",
    "                valid_indices = [i for i, t in enumerate(texts) if t.strip()]  # Remove espaços em branco\n",
    "                texts = [texts[i] for i in valid_indices]\n",
    "                metadatas = [metadatas[i] for i in valid_indices]\n",
    "                ids = [ids[i] for i in valid_indices]\n",
    "\n",
    "                if not texts:\n",
    "                    print(\"Todos os textos do lote estão vazios, pulando lote.\")\n",
    "                    continue\n",
    "\n",
    "                # Gerar embeddings em lote\n",
    "                try:\n",
    "                    embeddings = model.encode(texts, convert_to_tensor=False)  # Importante: convert_to_tensor=False\n",
    "                    embeddings = np.array(embeddings)  # Converter para NumPy array\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao gerar embeddings: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "                # *** CHECAR SE HÁ NaN NOS EMBEDDINGS ***\n",
    "                if np.isnan(embeddings).any():\n",
    "                    print(f\"Aviso: Encontrados valores NaN nos embeddings do lote {i//batch_size}\")\n",
    "                    continue\n",
    "\n",
    "                # Adicionar à coleção\n",
    "                collection.add(\n",
    "                    documents=texts,\n",
    "                    embeddings=embeddings.tolist(),  # Converter para lista\n",
    "                    metadatas=metadatas,\n",
    "                    ids=ids\n",
    "                )\n",
    "\n",
    "                id_counter += len(batch_chunks)\n",
    "                print(f\"Adicionado lote de {len(batch_chunks)} chunks à coleção {collection.name}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar lote na coleção {collection.name}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Concluído modelo {model_name}\")\n",
    "\n",
    "    print(\"\\nProcessamento concluído!\")\n",
    "    return chunks_por_professor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6: Execução do Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Configurar modelos de embedding\n",
    "    models = setup_embedding_models()\n",
    "\n",
    "    # Configurar ChromaDB e coleções\n",
    "    base_dir = os.getcwd()\n",
    "    chroma_client, collections = setup_chromadb(base_dir, models)\n",
    "\n",
    "    # Processar todos os arquivos JSON\n",
    "    chunks_por_professor = process_json_files(base_dir, chroma_client, models, collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula: Listagem Simples de Coleções\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "\n",
    "def simple_list_collections(base_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Lista apenas os nomes das coleções existentes no ChromaDB.\n",
    "    \"\"\"\n",
    "    chroma_dir = Path(base_dir) / \"chroma_db\"\n",
    "    chroma_client = chromadb.PersistentClient(path=str(chroma_dir))\n",
    "\n",
    "    collections = chroma_client.list_collections()\n",
    "\n",
    "    if not collections:\n",
    "        print(\"Nenhuma coleção encontrada no ChromaDB.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nColeções encontradas:\")\n",
    "    for collection in collections:\n",
    "        print(f\"- {collection.name}\")\n",
    "    print(f\"\\nTotal: {len(collections)} coleções\")\n",
    "\n",
    "# Exemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = os.getcwd()  # ou especifique o diretório base\n",
    "    simple_list_collections(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_collections_content(base_dir: str):\n",
    "    \"\"\"\n",
    "    Verifica detalhadamente o conteúdo de cada coleção\n",
    "    \"\"\"\n",
    "    chroma_dir = Path(base_dir) / \"chroma_db\"\n",
    "    chroma_client = chromadb.PersistentClient(path=str(chroma_dir))\n",
    "\n",
    "    collections = chroma_client.list_collections()\n",
    "    print(f\"Total de coleções: {len(collections)}\")\n",
    "\n",
    "    for collection in collections:\n",
    "        print(f\"\\nColeção: {collection.name}\")\n",
    "        try:\n",
    "            # Tentar obter a contagem de documentos\n",
    "            count = collection.count()\n",
    "            print(f\"Número de documentos: {count}\")\n",
    "\n",
    "            if count > 0:\n",
    "                # Tentar obter alguns documentos de exemplo\n",
    "                results = collection.get(limit=5)\n",
    "                print(\"\\nPrimeiros 5 documentos:\")\n",
    "                for i, (doc, meta) in enumerate(zip(results['documents'], results['metadatas'])):\n",
    "                    print(f\"\\nDocumento {i+1}:\")\n",
    "                    print(f\"Texto: {doc[:100]}...\")  # Primeiros 100 caracteres\n",
    "                    print(f\"Metadata: {meta}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao acessar coleção: {str(e)}\")\n",
    "\n",
    "# Execute esta função para verificar o conteúdo\n",
    "check_collections_content(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "import traceback\n",
    "\n",
    "def visualize_embeddings_pca(base_dir: str, interactive: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Visualiza os embeddings de todas as coleções usando PCA.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        chroma_dir = Path(base_dir) / \"chroma_db\"\n",
    "        chroma_client = chromadb.PersistentClient(path=str(chroma_dir))\n",
    "\n",
    "        collections = chroma_client.list_collections()\n",
    "        n_collections = len(collections)\n",
    "\n",
    "        if n_collections == 0:\n",
    "            print(\"Nenhuma coleção encontrada!\")\n",
    "            return\n",
    "\n",
    "        # Configurar o layout dos subplots\n",
    "        n_rows = (n_collections + 1) // 2\n",
    "        n_cols = 2 if n_collections > 1 else 1\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=n_rows,\n",
    "            cols=n_cols,\n",
    "            subplot_titles=[col.name.replace('horarios-professores_', '') for col in collections],\n",
    "            vertical_spacing=0.15,\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "\n",
    "        valid_collections = 0\n",
    "\n",
    "        for idx, collection in enumerate(collections, 1):\n",
    "            print(f\"\\nProcessando coleção: {collection.name}\")\n",
    "\n",
    "            try:\n",
    "                # Obter contagem de documentos\n",
    "                count = collection.count()\n",
    "                print(f\"Número total de documentos: {count}\")\n",
    "\n",
    "                if count == 0:\n",
    "                    print(f\"Aviso: Coleção {collection.name} está vazia\")\n",
    "                    continue\n",
    "\n",
    "                # Obter todos os documentos\n",
    "                results = collection.get(\n",
    "                    include=['embeddings', 'documents', 'metadatas'],\n",
    "                    limit=count\n",
    "                )\n",
    "\n",
    "                embeddings = results['embeddings']\n",
    "                metadatas = results['metadatas']\n",
    "\n",
    "                print(f\"Número de embeddings recuperados: {len(embeddings)}\")\n",
    "\n",
    "                # Verificar o primeiro embedding para debug\n",
    "                if len(embeddings) > 0:\n",
    "                    print(f\"Dimensões do primeiro embedding: {np.array(embeddings[0]).shape}\")\n",
    "\n",
    "                # Converter embeddings para numpy arrays e filtrar inválidos\n",
    "                valid_embeddings = []\n",
    "                valid_metadatas = []\n",
    "\n",
    "                for i, (emb, meta) in enumerate(zip(embeddings, metadatas)):\n",
    "                    try:\n",
    "                        arr = np.array(emb, dtype=np.float64)\n",
    "                        if arr.ndim == 1 and arr.size > 0 and not np.isnan(arr).any():\n",
    "                            valid_embeddings.append(arr)\n",
    "                            valid_metadatas.append(meta)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erro ao processar embedding {i}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "                if not valid_embeddings:\n",
    "                    print(f\"Nenhum embedding válido encontrado na coleção {collection.name}\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Embeddings válidos encontrados: {len(valid_embeddings)}\")\n",
    "\n",
    "                # Converter para array 2D\n",
    "                embeddings_array = np.vstack(valid_embeddings)\n",
    "                print(f\"Shape do array de embeddings: {embeddings_array.shape}\")\n",
    "\n",
    "                # Aplicar PCA\n",
    "                pca = PCA(n_components=2)\n",
    "                embeddings_2d = pca.fit_transform(embeddings_array)\n",
    "\n",
    "                # Calcular variância explicada\n",
    "                var_ratio = pca.explained_variance_ratio_\n",
    "                var_explained = f\"Variância explicada: {var_ratio[0]:.2%} (PC1), {var_ratio[1]:.2%} (PC2)\"\n",
    "\n",
    "                # Criar DataFrame\n",
    "                df = pd.DataFrame({\n",
    "                    'PC1': embeddings_2d[:, 0],\n",
    "                    'PC2': embeddings_2d[:, 1],\n",
    "                    'professor': [m['professor'] for m in valid_metadatas],\n",
    "                    'tipo': [m.get('type', 'info_professor') for m in valid_metadatas],\n",
    "                    'departamento': [m['departamento'] for m in valid_metadatas],\n",
    "                    'disciplina': [m.get('nome_disciplina', '') for m in valid_metadatas]\n",
    "                })\n",
    "\n",
    "                # Calcular posição no subplot\n",
    "                valid_collections += 1\n",
    "                row = (valid_collections - 1) // 2 + 1\n",
    "                col = 2 if valid_collections % 2 == 0 else 1\n",
    "\n",
    "                # Criar scatter plot\n",
    "                scatter = go.Scatter(\n",
    "                    x=df['PC1'],\n",
    "                    y=df['PC2'],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=8,\n",
    "                        color=pd.factorize(df['tipo'])[0],\n",
    "                        colorscale='Viridis',\n",
    "                        showscale=False\n",
    "                    ),\n",
    "                    text=[f\"Professor: {p}<br>Departamento: {d}<br>Tipo: {t}<br>Disciplina: {disc}\"\n",
    "                          for p, d, t, disc in zip(df['professor'], df['departamento'], df['tipo'], df['disciplina'])],\n",
    "                    hoverinfo='text',\n",
    "                    name=collection.name\n",
    "                )\n",
    "\n",
    "                fig.add_trace(scatter, row=row, col=col)\n",
    "\n",
    "                # Adicionar texto com variância explicada\n",
    "                fig.add_annotation(\n",
    "                    text=var_explained,\n",
    "                    xref=f\"x{valid_collections}\", yref=f\"y{valid_collections}\",\n",
    "                    x=0.5, y=-0.15,\n",
    "                    showarrow=False,\n",
    "                    font=dict(size=10),\n",
    "                    xanchor='center'\n",
    "                )\n",
    "\n",
    "                # Atualizar layout dos eixos\n",
    "                fig.update_xaxes(title_text=\"PC1\", row=row, col=col)\n",
    "                fig.update_yaxes(title_text=\"PC2\", row=row, col=col)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar coleção {collection.name}:\")\n",
    "                print(traceback.format_exc())\n",
    "                continue\n",
    "\n",
    "        if valid_collections == 0:\n",
    "            print(\"\\nNenhuma coleção válida encontrada para visualização!\")\n",
    "            return\n",
    "\n",
    "        # Atualizar layout geral\n",
    "        fig.update_layout(\n",
    "            title_text=\"Visualização de Embeddings (PCA)\",\n",
    "            height=400 * ((valid_collections + 1) // 2),\n",
    "            width=1200,\n",
    "            showlegend=False,\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "\n",
    "        # Salvar e mostrar\n",
    "        try:\n",
    "            fig.write_html(\"embeddings_visualization_pca.html\")\n",
    "            print(\"\\nVisualização salva em embeddings_visualization_pca.html\")\n",
    "\n",
    "            if not interactive:\n",
    "                try:\n",
    "                    import kaleido\n",
    "                    fig.write_image(\"embeddings_visualization_pca.png\", scale=2)\n",
    "                    print(\"Visualização salva em embeddings_visualization_pca.png\")\n",
    "                except ImportError:\n",
    "                    print(\"Para salvar como PNG, instale o pacote kaleido: pip install -U kaleido\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao salvar visualizações: {str(e)}\")\n",
    "\n",
    "        if interactive:\n",
    "            fig.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Erro geral na função:\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "# Chamada direta da função\n",
    "base_dir = os.getcwd()\n",
    "visualize_embeddings_pca(base_dir, interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d678dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_embeddings_pca_turma(base_dir: str, modelo: str = None, interactive: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Visualiza os embeddings de uma coleção usando PCA, colorindo por turma.\n",
    "    Se modelo for None, usa a primeira coleção encontrada.\n",
    "    \"\"\"\n",
    "    chroma_dir = Path(base_dir) / \"chroma_db\"\n",
    "    chroma_client = chromadb.PersistentClient(path=str(chroma_dir))\n",
    "\n",
    "    # Seleciona a coleção\n",
    "    collections = chroma_client.list_collections()\n",
    "    if not collections:\n",
    "        print(\"Nenhuma coleção encontrada!\")\n",
    "        return\n",
    "\n",
    "    if modelo:\n",
    "        collection = next((c for c in collections if modelo in c.name), None)\n",
    "        if not collection:\n",
    "            print(f\"Modelo '{modelo}' não encontrado nas coleções.\")\n",
    "            return\n",
    "    else:\n",
    "        collection = collections[0]\n",
    "        print(f\"Usando coleção: {collection.name}\")\n",
    "\n",
    "    # Obter todos os documentos\n",
    "    count = collection.count()\n",
    "    results = collection.get(include=['embeddings', 'metadatas'], limit=count)\n",
    "    embeddings = results['embeddings']\n",
    "    metadatas = results['metadatas']\n",
    "\n",
    "    # Filtrar embeddings válidos\n",
    "    valid_embeddings = []\n",
    "    valid_metadatas = []\n",
    "    for emb, meta in zip(embeddings, metadatas):\n",
    "        arr = np.array(emb)\n",
    "        if arr.ndim == 1 and arr.size > 0 and not np.isnan(arr).any():\n",
    "            valid_embeddings.append(arr)\n",
    "            valid_metadatas.append(meta)\n",
    "\n",
    "    if not valid_embeddings:\n",
    "        print(\"Nenhum embedding válido encontrado.\")\n",
    "        return\n",
    "\n",
    "    embeddings_array = np.vstack(valid_embeddings)\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embeddings_array)\n",
    "\n",
    "    # Criar DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'PC1': embeddings_2d[:, 0],\n",
    "        'PC2': embeddings_2d[:, 1],\n",
    "        'professor': [m['professor'] for m in valid_metadatas],\n",
    "        'departamento': [m['departamento'] for m in valid_metadatas],\n",
    "        'tipo': [m.get('type', 'info_professor') for m in valid_metadatas],\n",
    "        'disciplina': [m.get('nome_disciplina', '') for m in valid_metadatas],\n",
    "        'turma': [m.get('turma', 'desconhecida') for m in valid_metadatas]  # <-- aqui!\n",
    "    })\n",
    "\n",
    "    # Visualização com cor por turma\n",
    "    fig = px.scatter(\n",
    "        df, x='PC1', y='PC2',\n",
    "        color='departamento',\n",
    "        hover_data=['professor', 'departamento', 'tipo', 'disciplina', 'turma'],\n",
    "        title=f'Embeddings PCA colorido por Turma ({collection.name})'\n",
    "    )\n",
    "\n",
    "    if interactive:\n",
    "        fig.show()\n",
    "    else:\n",
    "        fig.write_html(\"embeddings_pca_turma.html\")\n",
    "        print(\"Visualização salva em embeddings_pca_turma.html\")\n",
    "\n",
    "# Exemplo de uso:\n",
    "base_dir = os.getcwd()\n",
    "# Se quiser um modelo específico, passe o nome (ou parte do nome) em modelo=...\n",
    "# Exemplo: modelo=\"distiluse-base-multilingual-cased-v2\"\n",
    "visualize_embeddings_pca_turma(base_dir, modelo=None, interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula: Listar todos os metadados com o campo 'nome_disciplina' da coleção horarios-professores_all-MiniLM-L6-v2\n",
    "\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Defina o diretório base e inicialize o cliente ChromaDB\n",
    "base_dir = os.getcwd()\n",
    "chroma_dir = Path(base_dir) / \"chroma_db\"\n",
    "chroma_client = chromadb.PersistentClient(path=str(chroma_dir))\n",
    "\n",
    "# Nome da coleção desejada\n",
    "collection_name = \"horarios-professores_all-MiniLM-L6-v2\"\n",
    "\n",
    "try:\n",
    "    collection = chroma_client.get_collection(collection_name)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar a coleção: {e}\")\n",
    "    collection = None\n",
    "\n",
    "if collection:\n",
    "    count = collection.count()\n",
    "    print(f\"Total de documentos na coleção: {count}\")\n",
    "\n",
    "    # Buscar todos os metadados\n",
    "    results = collection.get(include=['metadatas'], limit=count)\n",
    "    metadatas = results['metadatas']\n",
    "\n",
    "    disciplinas = []\n",
    "    for meta in metadatas:\n",
    "        if 'nome_disciplina' in meta:\n",
    "            disciplinas.append(meta['nome_disciplina'])\n",
    "\n",
    "    print(f\"\\nTotal de metadados com 'nome_disciplina': {len(disciplinas)}\")\n",
    "    for i, nome in enumerate(disciplinas, 1):\n",
    "        print(f\"{i}: {nome}\")\n",
    "else:\n",
    "    print(\"Coleção não encontrada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula: Listar todos os professores únicos da coleção horarios-professores_all-MiniLM-L6-v2\n",
    "\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Defina o diretório base e inicialize o cliente ChromaDB\n",
    "base_dir = os.getcwd()\n",
    "chroma_dir = Path(base_dir) / \"chroma_db\"\n",
    "chroma_client = chromadb.PersistentClient(path=str(chroma_dir))\n",
    "\n",
    "# Nome da coleção desejada\n",
    "collection_name = \"horarios-professores_all-MiniLM-L6-v2\"\n",
    "\n",
    "try:\n",
    "    collection = chroma_client.get_collection(collection_name)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar a coleção: {e}\")\n",
    "    collection = None\n",
    "\n",
    "if collection:\n",
    "    count = collection.count()\n",
    "    print(f\"Total de documentos na coleção: {count}\")\n",
    "\n",
    "    # Buscar todos os metadados\n",
    "    results = collection.get(include=['metadatas'], limit=count)\n",
    "    metadatas = results['metadatas']\n",
    "\n",
    "    professores = set()\n",
    "    for meta in metadatas:\n",
    "        if 'professor' in meta:\n",
    "            professores.add(meta['professor'])\n",
    "\n",
    "    print(f\"\\nTotal de professores únicos: {len(professores)}\")\n",
    "    for i, nome in enumerate(sorted(professores), 1):\n",
    "        print(f\"{i}: {nome}\")\n",
    "else:\n",
    "    print(\"Coleção não encontrada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03a0ac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Coleta: horarios-professores_all-MiniLM-L6-v2\n",
      "================================================================================\n",
      "Total de chunks encontrados para o professor 'Evando Carlos Pizzini': 5\n",
      "\n",
      "Chunk 1:\n",
      "Texto: O professor Evando Carlos Pizzini ministra a disciplina Fundamentos De Estruturas De Dados (COM1009) para a turma CC2 no formato Presencial.\n",
      "Horários:\n",
      "- Quinta-feira: m4 (10:20-11:10), m5 (11:10-12:00) - Sala I25\n",
      "- Terca-feira: m4 (10:20-11:10), m5 (11:10-12:00) - Sala I25\n",
      "Metadados: {'turma': 'CC2', 'professor': 'Evando Carlos Pizzini', 'nome_disciplina': 'Fundamentos De Estruturas De Dados', 'periodo': '2025/1', 'departamento': 'DACOM', 'codigo': 'COM1009'}\n",
      "------------------------------------------------------------\n",
      "Chunk 2:\n",
      "Texto: O professor Evando Carlos Pizzini pertence ao departamento DACOM e está ativo no período 2025/1.\n",
      "Metadados: {'professor': 'Evando Carlos Pizzini', 'departamento': 'DACOM', 'periodo': '2025/1'}\n",
      "------------------------------------------------------------\n",
      "Chunk 3:\n",
      "Texto: O professor Evando Carlos Pizzini ministra a disciplina Aspectos Formais Da Computação (CC56E) para a turma CC6 no formato Presencial.\n",
      "Horários:\n",
      "- Segunda-feira: m4 (10:20-11:10), m5 (11:10-12:00) - Sala L15\n",
      "- Terca-feira: m1 (07:30-08:20) - Sala L15\n",
      "Metadados: {'departamento': 'DACOM', 'periodo': '2025/1', 'professor': 'Evando Carlos Pizzini', 'turma': 'CC6', 'codigo': 'CC56E', 'nome_disciplina': 'Aspectos Formais Da Computação'}\n",
      "------------------------------------------------------------\n",
      "Chunk 4:\n",
      "Texto: O professor Evando Carlos Pizzini ministra a disciplina Árvores E Grafos (COM1014) para a turma CC3 no formato Presencial.\n",
      "Horários:\n",
      "- Quinta-feira: t3 (14:40-15:30), t4 (15:50-16:40) - Sala L18\n",
      "- Terca-feira: t4 (15:50-16:40), t5 (16:40-17:30) - Sala L18\n",
      "Metadados: {'periodo': '2025/1', 'nome_disciplina': 'Árvores E Grafos', 'codigo': 'COM1014', 'turma': 'CC3', 'departamento': 'DACOM', 'professor': 'Evando Carlos Pizzini'}\n",
      "------------------------------------------------------------\n",
      "Chunk 5:\n",
      "Texto: O professor Evando Carlos Pizzini ministra a disciplina Teoria Da Computação (COM1025) para a turma CC5 no formato Presencial.\n",
      "Horários:\n",
      "- Quinta-feira: t1 (13:00-13:50), t2 (13:50-14:40) - Sala L15\n",
      "- Segunda-feira: t1 (13:00-13:50), t2 (13:50-14:40), t3 (14:40-15:30) - Sala L15\n",
      "Metadados: {'professor': 'Evando Carlos Pizzini', 'codigo': 'COM1025', 'periodo': '2025/1', 'turma': 'CC5', 'nome_disciplina': 'Teoria Da Computação', 'departamento': 'DACOM'}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Célula: Exibir todos os chunks das coleções para o professor\n",
    "\n",
    "professor_nome = \"Evando Carlos Pizzini\"\n",
    "colecoes = [\n",
    "    (\"horarios-professores_all-MiniLM-L6-v2\", 384)\n",
    "]\n",
    "\n",
    "for collection_name, emb_size in colecoes:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Coleta: {collection_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    try:\n",
    "        collection = chroma_client.get_collection(collection_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar a coleção: {e}\")\n",
    "        continue\n",
    "\n",
    "    if collection:\n",
    "        results = collection.query(\n",
    "            query_embeddings=[[0.0]*emb_size],  # Dummy embedding\n",
    "            where={\"professor\": professor_nome},\n",
    "            n_results=1000,\n",
    "            include=[\"documents\", \"metadatas\"]\n",
    "        )\n",
    "\n",
    "        docs = results.get(\"documents\", [[]])[0]\n",
    "        metas = results.get(\"metadatas\", [[]])[0]\n",
    "\n",
    "        print(f\"Total de chunks encontrados para o professor '{professor_nome}': {len(docs)}\\n\")\n",
    "        for i, (doc, meta) in enumerate(zip(docs, metas), 1):\n",
    "            print(f\"Chunk {i}:\")\n",
    "            print(\"Texto:\", doc.strip())\n",
    "            print(\"Metadados:\", meta)\n",
    "            print(\"-\" * 60)\n",
    "    else:\n",
    "        print(\"Coleção não encontrada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
